<?xml version="1.0" encoding="UTF-8" standalone="no"?><?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/rss2full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" version="2.0">

<channel>
	<title>Salesforce Developers Blogs</title>
	
	<link>https://developer.salesforce.com/blogs</link>
	<description>Elevating developer skills and connecting with the Salesforce Developers community</description>
	<lastBuildDate>Tue, 16 Dec 2014 11:30:38 +0000</lastBuildDate>
	<language>en-US</language>
		<sy:updatePeriod>hourly</sy:updatePeriod>
		<sy:updateFrequency>1</sy:updateFrequency>
	
	<atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" href="http://feeds.feedburner.com/SforceBlog" rel="self" type="application/rss+xml"/><feedburner:info uri="sforceblog"/><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" href="http://pubsubhubbub.appspot.com/" rel="hub"/><feedburner:emailServiceId>SforceBlog</feedburner:emailServiceId><feedburner:feedburnerHostname>https://feedburner.google.com</feedburner:feedburnerHostname><feedburner:feedFlare href="http://add.my.yahoo.com/rss?url=http%3A%2F%2Ffeeds.feedburner.com%2FSforceBlog" src="http://us.i1.yimg.com/us.yimg.com/i/us/my/addtomyyahoo4.gif">Subscribe with My Yahoo!</feedburner:feedFlare><feedburner:feedFlare href="http://www.newsgator.com/ngs/subscriber/subext.aspx?url=http%3A%2F%2Ffeeds.feedburner.com%2FSforceBlog" src="http://www.newsgator.com/images/ngsub1.gif">Subscribe with NewsGator</feedburner:feedFlare><feedburner:feedFlare href="http://feeds.my.aol.com/add.jsp?url=http%3A%2F%2Ffeeds.feedburner.com%2FSforceBlog" src="http://o.aolcdn.com/favorites.my.aol.com/webmaster/ffclient/webroot/locale/en-US/images/myAOLButtonSmall.gif">Subscribe with My AOL</feedburner:feedFlare><feedburner:feedFlare href="http://www.bloglines.com/sub/http://feeds.feedburner.com/SforceBlog" src="http://www.bloglines.com/images/sub_modern11.gif">Subscribe with Bloglines</feedburner:feedFlare><feedburner:feedFlare href="http://www.netvibes.com/subscribe.php?url=http%3A%2F%2Ffeeds.feedburner.com%2FSforceBlog" src="http://www.netvibes.com/img/add2netvibes.gif">Subscribe with Netvibes</feedburner:feedFlare><feedburner:feedFlare href="http://fusion.google.com/add?feedurl=http%3A%2F%2Ffeeds.feedburner.com%2FSforceBlog" src="http://buttons.googlesyndication.com/fusion/add.gif">Subscribe with Google</feedburner:feedFlare><feedburner:feedFlare href="http://www.pageflakes.com/subscribe.aspx?url=http%3A%2F%2Ffeeds.feedburner.com%2FSforceBlog" src="http://www.pageflakes.com/ImageFile.ashx?instanceId=Static_4&amp;fileName=ATP_blu_91x17.gif">Subscribe with Pageflakes</feedburner:feedFlare><item>
		<title>The Gift of Code</title>
		<link>http://feedproxy.google.com/~r/SforceBlog/~3/PKWxzwWfpjY/gift-code-2.html</link>
		<comments>http://developer.salesforce.com/blogs/developer-relations/2014/12/gift-code-2.html#comments</comments>
		<pubDate>Tue, 16 Dec 2014 11:23:32 +0000</pubDate>
		<dc:creator><![CDATA[Peter Chittum]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://26.92491</guid>
		<description><![CDATA[<a href="http://developer.salesforce.com/blogs/developer-relations/2014/12/gift-code-2.html"><img align="right" hspace="5" width="150" src="http://res.cloudinary.com/hzxejch6p/image/upload/c_scale,w_278/v1418728748/gift_PNG5982_o2tjdu.png" class="alignright wp-post-image tfe" alt="Gift" title="Gift" /></a>You are a developer. You build applications. Those are valuable skills. But these skills can benefit more than your career. Whether you&#8217;re an experienced Salesforce Developer, or just learning the ropes, your expertise can also benefit our thousands of Salesforce Foundation customers who need help customizing their Salesforce org. Why not give some of your time [&#8230;]]]></description>
				<content:encoded><![CDATA[<p><img class="alignright wp-image-92591" title="Gift" alt="Gift" src="http://res.cloudinary.com/hzxejch6p/image/upload/c_scale,w_278/v1418728748/gift_PNG5982_o2tjdu.png" width="278" height="222" />You are a developer. You build applications. Those are valuable skills.</p>
<p>But these skills can benefit more than your career. Whether you&#8217;re an experienced Salesforce Developer, or just learning the ropes, your expertise can also benefit our thousands of Salesforce Foundation customers who need help customizing their Salesforce org. Why not give some of your time and become a pro bono app developer?</p>
<h1>Salesforce Foundation</h1>
<p>We at Salesforce are not quiet about our charitable arm, the <a title="Salesforce Foundation" href="http://www.salesforcefoundation.org/" target="_blank">Salesforce Foundation</a>. Nor are we quiet about the 1-1-1 model of giving away 1% of our equity, product, and employee time. For many who join Salesforce, this is one factor we considered when joining: that Salesforce enables and encourages us to give back to our community by giving us paid volunteer time off (VTO). There are many ways which we use VTO, one of which is pro bono project work for Foundation customers.</p>
<h1>Pro Bono Projects</h1>
<p>When I was new to developing Salesforce solutions, one way I used to enhance my experience was on pro bono projects. I found it so beneficial I often suggest to other developers who are just learning doing pro bono project work to help gain the necessary experience to master the Salesforce1 platform technologies.</p>
<p>I was fortunate to have my colleagues and the Salesforce Foundation employees to help me find such projects. But it never occurred to me that it might not always be obvious how to find a pro bono project if you don&#8217;t have such resources around.</p>
<h1>Coding for a Good Cause</h1>
<p>Thankfully, there are a host of organizations these days that are there to help IT experts who want to give their time and expertise to non-profits, charities, schools, and NGO&#8217;s. A good place to start is the <a title="Pro Bono Page" href="http://www.salesforcefoundation.org/volunteers/probono/" target="_blank">Salesforce Foundation&#8217;s pro bono page</a>.</p>
<p>On this page you will find a list of organizations whose purpose is to match developers to organizations who may need their help. Some are primarily for non-profit organizations based in the United States. If you&#8217;re based elsewhere and want to be connected to something more local, I&#8217;m told Taproot is one organization to look into. You can also list your interests on your LinkedIn profile.</p>
<p>In addition to connecting with one of these organizations, the pro-bono page also has a button you can click on to pledge your time over the coming year and commit to a number of hours of pro bono work.</p>
<p>None of these organizations are specific to Salesforce only. Any developer with any technology can sign up to give their time.</p>
<h1>Project Scope</h1>
<p><span style="line-height: 1.5em">Having done a few pro bono projects, I can say first hand that getting your scope right is essential for pro bono Foundation customer projects.</span></p>
<p>In my case, I had a limited amount of volunteer time I needed to arrange. The Salesforce VTO is only 48 hours per year, so I had to be certain that whatever project I did would fit into that time.</p>
<p>There are no hard and fast rules, but the formula that has worked for me is that in each case, I worked with organizations that already had good Salesforce administration skills. Each project allowed me to fill a small gap in functionality that the Foundation customer didn&#8217;t have the skills to build. Here are some examples of projects I have worked on:</p>
<ul>
<li>A custom Visualforce page that would promote students to their next year&#8217;s class at the end of each academic year.</li>
<li>Some Apex back-end logic that would ensure that a taxonomy database had correct bi-directional links between classifications.</li>
<li>Trigger logic that would create a roll-up count of Attachment records for any object (<a title="Attachment Rollup Count" href="https://github.com/pchittum/AttachmentRollup">get the code here</a>).</li>
</ul>
<p>You can see how in each instance, the requirements were finite and specific. My work served as a complement to the  work that was being done by some existing expert for each customer.</p>
<p>In my case all solutions required coding, but there is nothing saying that non-programmers cannot use this same approach. Many of our Foundation customers have one person who is the administrator, developer, report builder, etc. So just having another expert to work with can enable that one-man-band to deliver more to their business.</p>
<p>O<span style="line-height: 1.5em">k&#8230;let&#8217;s be honest, though. Setting a proper time and scope are not unique to pro bono projects. But there are some unique risks when you are giving your time for free. </span><span style="line-height: 1.5em">So take extra care here in order that your pro bono project provides everyone with the maximum in satisfaction. </span></p>
<h1>Benefits for All Parties</h1>
<p>In addition to providing functionality that helped each organization better serve their charitable aims, I can point to specific skills that each solution taught me. For instance the taxonomy database solution forced me to implement the recursive trigger pattern. This taught me a lot about managing trigger state when a trigger could potentially invoke itself.</p>
<p>In the case of the Attachment roll-up, the original requirement was for this feature to be implemented for a single parent object. But since Attachments exist as a child on almost every top-level object, I expanded my solution so that it could work on any parent object. I then took the code and made it into an <a title="Attachment Rollup" href="https://github.com/pchittum/AttachmentRollup">open source project</a> published on github, and an unmanaged package.</p>
<h1>Continuing the Relationship</h1>
<p>After initially working on a project, it can also be valuable to continue the relationship. Often your experience in other projects can be brought to bear on technological problems that arise for charities as they create their Salesforce solution. A number of us work with foundation customers on an ongoing basis. It can be valuable to the customer, as they have a real flesh and blood person to ask questions to. It can also be valuable to you as a developer as you might be exposed to problems that you might not have otherwise encountered.</p>
<p>The other advantage of an ongoing relationship is that you become a trusted counsel, and begin to understand the needs of that organization. If you are given a full time login to that org, and contribute regularly enough, you might even consider joining the Salesforce Foundation online Community.</p>
<h1>How&#8217;s That Go Again?</h1>
<p>To sum up here are the things you should do to get started on your own pro bono Salesforce Foundation project:</p>
<ol>
<li>Check out the Salesforce Foundation Pro Bono Page</li>
<li><span style="line-height: 1.5em">Connect with a Salesforce Foundation Customer</span></li>
<li><span style="line-height: 1.5em">Set a finite project scope</span></li>
<li>Get to work!</li>
<li>Build a relationship with your Foundation customer</li>
</ol>
<p>So what are you waiting for?</p>
<p>Here&#8217;s hoping that you find a great Foundation customer match. And that everyone has a happy, healthy, and generous end of 2014. And that it carries on into 2015.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/SforceBlog?a=PKWxzwWfpjY:VYEmr6NDDkQ:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=PKWxzwWfpjY:VYEmr6NDDkQ:qj6IDK7rITs"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=qj6IDK7rITs" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=PKWxzwWfpjY:VYEmr6NDDkQ:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=PKWxzwWfpjY:VYEmr6NDDkQ:V_sGLiPBpWU" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=PKWxzwWfpjY:VYEmr6NDDkQ:F7zBnMyn0Lo"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=PKWxzwWfpjY:VYEmr6NDDkQ:F7zBnMyn0Lo" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=PKWxzwWfpjY:VYEmr6NDDkQ:l6gmwiTKsz0"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=l6gmwiTKsz0" border="0"></img></a>
</div><img src="//feeds.feedburner.com/~r/SforceBlog/~4/PKWxzwWfpjY" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://developer.salesforce.com/blogs/developer-relations/2014/12/gift-code-2.html/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		<feedburner:origLink>http://developer.salesforce.com/blogs/developer-relations/2014/12/gift-code-2.html</feedburner:origLink></item>
		<item>
		<title>Salesforce1 Enterprise Deployment Strategy</title>
		<link>http://feedproxy.google.com/~r/SforceBlog/~3/sXxZv4f5LWQ/salesforce1-enterprise-deployment-strategy.html</link>
		<comments>http://developer.salesforce.com/blogs/developer-relations/2014/12/salesforce1-enterprise-deployment-strategy.html#comments</comments>
		<pubDate>Mon, 15 Dec 2014 19:17:35 +0000</pubDate>
		<dc:creator><![CDATA[Greg Cook]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Enterprise Architecture]]></category>

		<guid isPermaLink="false">http://26.91941</guid>
		<description><![CDATA[Learn how to plan, rehearse, and execute on your next flawless Salesforce1 enterprise deployment.]]></description>
				<content:encoded><![CDATA[<p><em>Editors Note: This post is part of a “Guest” series entitled Enterprise Architecture with Force.com. Our <em>guest blogger, </em>Greg Cook, is a managing partner of CloudPremise and currently holds all seven Salesforce certifications.</em></p>
<p><img class="alignright wp-image-40601" title="Enterprise Architecture with Force.com" alt="" src="https://res.cloudinary.com/hzxejch6p/image/upload/v1418671315/Greg_Cookx150_dmqjxj.jpg" width="122" height="150" />In my recent article<span style="color: #000000;"> on <span style="color: #0000ff;"><a title="Salesforce1 Environment Management" href="https://developer.salesforce.com/blogs/developer-relations/2014/12/salesforce1-enterprise-environment-management.html" target="_blank"><span style="color: #0000ff;">Environment Management </span></a></span>I expl</span>ained some of the intricacies related to managing the Salesforce1 Platform metadata. However, regardless of the number of environments you maintain, there are some critical techniques necessary for managing a Salesforce1 Enterprise deployment.</p>
<p>For the purposes of this article, &#8220;deployment&#8221; will be defined as the steps necessary for completing a smooth roll-out of new functionality into your production org.  It does NOT include some of the user-adoption related issues like training and support (which are just as important if not MORE). Let&#8217;s look at a successful migration and the people, tools, and processes necessary for complete a &#8220;flawless&#8221; enterprise deployment.</p>
<h2>People</h2>
<p>The following roles are necessary for executing a smooth deployment.  Understand that these are &#8220;roles&#8221; and not &#8220;resources,&#8221; and that some people might wear multiple hats.  In large projects there might be multiple people in these roles.  In smaller projects there might be one person playing ALL of these roles:</p>
<ol>
<li><strong>Environment Manager</strong> - I recommend a dedicated technical administrator whose responsibility is &#8220;Environment Integrity.&#8221;  I have heard many different names describing this role (build master, deployment manager, configuration management engineer, etc.). However regardless of the name, there is a very important concept: Developers shouldn&#8217;t have the privileges or responsibilities to deploy their own code into Production! Call me old-fashioned, however my urgency around this principle not only includes any necessary compliance requirements; it also includes taking the time to <strong>PLAN, REHEARSE, and EXECUTE</strong> your deployments.  When Salesforce developers deploy their own code it often leads to <a href="http://en.wikipedia.org/wiki/Cowboy_coding" target="_blank">cowboy coding</a> practices, which is exactly the thing we are trying to avoid.  The environment manager will actually execute the steps outlined in the deployment plan, including manual configuration steps, and utilizing the source control and deployment tools.</li>
<li><strong>Release Manager</strong> - The release manager is the functional counter-part to the environment manager.  The release manager is the owner of the release calendar and the necessary flow of information between the technical teams and the users (the Communication Plan).  The release manager is also responsible for building and distributing a thorough set of release notes.</li>
<li><strong>Development Team - </strong>As features are designed and built in the development environments, developers are responsible for documenting their deployment steps AS THEY BUILD.  Successful deployments start during development.  If you wait to define your deployment steps until testing is complete, you are most likely going to have a very painful migration.</li>
<li><strong>Business Stakeholder(s) - </strong>Business stakeholders need to have visibility and authority in the release process.</li>
<li><strong>IT Stakeholder(s) - </strong>Internal IT stakeholders, such as technical architects, project managers, and the change management team, need to be fully aware of any Salesforce deployments.</li>
</ol>
<h2>Tools</h2>
<p>The tools listed below range between sophisticated software and simple documents - however all are vital components of your deployment strategy.</p>
<ol>
<li><strong>Release Calendar</strong> - This document maintains a record of all upcoming changes to your Salesforce environment.  It should include details on sandbox activity (planned refreshes, features moving to test, etc.), as well as Salesforce release details (Pre-Release Sandbox Upgrades, Production Updates, etc.).  It should be used to communicate with stakeholders as well and manage any necessary deployment moratoriums.  The release manager should own and maintain this document.</li>
<li><strong>Configuration Workbook</strong> - Complex Salesforce environments need living design documentation.  I call this the configuration workbook.  This can be a wiki, shared spreadsheet, or complex configuration management tool.  Regardless of your method, developers, architects, admins, and environment managers should have a place where they can maintain details about each component of the Salesforce environment.  The configuration workbook can be used to track the lifecycle of components (proposed, designing, testing, released, retired), and should be maintained throughout the development and deployment activities.</li>
<li><strong>Deployment Package</strong> - A deployment package contains all of the components necessary for executing a migration to a new environment.  It typically consists of multiple components including the deployment manifest, deployment plan, and release notes.</li>
<li><strong>Deployment Manifest</strong> - A deployment manifest is a catalog (list) of all of the components to be migrated through the API.  A change-set is also considered a deployment manifest.  If you are using the ANT Migration toolkit, the manifest is your package.xml file.  It can also be a simple spreadsheet.</li>
<li><strong>Deployment Plan</strong> - The deployment plan should list the deployment manifest(s) necessary for migration, as well as any and all manual steps.  The deployment plan should be constructed by the development team and executed by your environment manager.</li>
<li><strong>Release Notes - </strong>A successful deployment should include a functional description of the new and changed functionality.  These release notes should be built during the development process and distributed as part of the release by the release manager.</li>
<li><strong>Communication Plan </strong>- Another key element of a deployment is an effective communication plan.  This can come via email, blog, or even Chatter.  Business and IT stakeholders must have continual transparency to release timelines in order to plan accordingly.  The release manager should be responsible for the communication plan.</li>
<li><strong>Source Control Tool - </strong>Source control should play an active role in your deployment process.  At a minimum, code should be versioned in the source control repository.  Sophisticated teams can use branching, merging, and continuous integration techniques to manage their code and configuration as well.  You can even include most of the documents in this list in your source control repository for versioning.</li>
<li><strong>Deployment Tool </strong>- Teams should decide on a specific tool (or tools) for deployment.  The obvious options are Salesforce Change Sets, Eclipse IDE or the ANT Migration tool.  However there are numerous third-party tools that can aid in this process.  Whatever the tool, ensure your team&#8217;s consistent and disciplined use of the same tool throughout all of your deployments.</li>
<li><strong>Data Loading Tool</strong> - Data loading is a critical aspect of deployments.  Tools for loading data can also be used to automate some processes or manual changes as well (i.e. user/profile changes, etc.).</li>
<li><strong>Metadata Comparison Tool - </strong>Having a good tool to analyze differences between orgs can be very helpful.  Verify and understand any differences between orgs as you migrate between them.</li>
<li><strong>Web Browser Testing Tool</strong> - Typically, Salesforce deployments can&#8217;t be completed 100% through the Metadata API or change sets.  However you can record and replay your manual deployment steps with a tool like Selenium or QTP to achieve 100% automation.</li>
<li><strong>Change Window</strong> - This is not a tool but more of a concept. The change window is a critical aspect of your deployment strategy to ensure predictability and trust.  Deployments to production should only occur during an approved change window that both business and IT stakeholders have agreed upon.</li>
</ol>
<h2>Processes</h2>
<p>A flawless deployment depends on the correct planning, rehearsal, and execution of the following steps (at a minimum!):</p>
<ol>
<li>
<h4>Plan</h4>
<ol>
<li>Add the release date(s) to the release calendar using your specific project methodology and business requirements.  Validate your targeted release dates against internal moratoriums and Salesforce&#8217;s own release calendar.  (I would NOT recommend planning a large release around Salesforce&#8217;s own release dates.  Give things a couple weeks to stabilize.)</li>
<li>Choose a deployment tool. (My preference is the <a href="https://developer.salesforce.com/page/Force.com_Migration_Tool" target="_blank">ANT Migration Tool</a> but there are many options here.) The important thing about your deployment tool is to be consistent in its use as you migrate your features through each environment.</li>
<li>Plan to create your deployment package AS you develop.  Changes that can be automated through the API should be listed in the deployment manifest.  Changes that cannot be automated should be documented with detailed instructions in your deployment plan.</li>
<li>Agree to formal Quality criteria for your release.  For example, agree that Zero Critical Defects will be allowed in order to release, while High Priority Defects are allowed in the release with business stakeholder approval, and perhaps 15% or lower Low Priority Defects are allowed in order to release.  Whatever the criteria is, formally document and agree upon this with your stakeholders.  On large projects this criteria is vital to maintaining a viable release date.</li>
<li>Decide on your environment migration path.  You can see my earlier po<span style="color: #000000;">st <a title="Salesforce Environment Management" href="https://developer.salesforce.com/blogs/developer-relations/2014/12/salesforce1-enterprise-environment-management.html" target="_blank">here</a></span> on a recommended environment plan.  Regardless of your path, determine which environment will be your &#8220;from&#8221; environment. (For example, System Test Full Sandbox or Staging Developer Pro Sandbox, etc.)</li>
<li>Determine your Formal Go/No-Go governance methodology.  Who needs to approve the deployment from the business side?  Who from IT?  What formal processes must be followed (Architecture Review Board, ITIL change management process, etc.) in your organization?</li>
<li>Determine your communication plan. Who from the business needs to be informed of the upcoming release?  How far in advance?  Who in IT needs to be informed and on what recurrence?  What about your users?  How will success (or failure) be communicated?  Plan this in advance to ensure appropriate visibility of the Salesforce changes.</li>
<li>If you are working with Salesforce&#8217;s Customers For Life program, communicate your release plans with your CFL team.  They can provide timely notifications to both you and Salesforce internally.</li>
</ol>
</li>
<li>
<h4>Rehearse (Rehearsal for production deployment begins as soon as development and configuration start on the release):</h4>
<ol>
<li>Add any necessary information to the deployment plan and/or deployment manifest AS EACH FEATURE IS DEVELOPED.  It is imperative that production deployment is considered as you build the feature.  For example: if the feature requires an Apex trigger, add it to the deployment manifest; if the feature requires manual changes to the security model, add those instructions to the deployment plan.</li>
<li>Create the appropriate release notes that can be compiled by the release manager AS EACH FEATURE IS DEVELOPED.</li>
<li>Add relevant information to your design docs and configuration workbook AS EACH FEATURE IS DEVELOPED.  This documentation is vital for maintaining complex environments with numerous parallel projects.</li>
<li>Version both your code and your configuration using source control tools.  Utilize branching and merging as necessary based upon your team-specific workflow.  Have developers document the feature number when they check in their code.</li>
<li>Before migrating features out of each development environment, your developers should have built their own feature-specific deployment packages.  Their code should be listed in a feature-specific deployment manifest.  Their manual instructions should be listed in a feature-specific deployment plan.  Their feature-specific release notes should ALREADY be written.</li>
<li>As the feature is migrated into a test environment, take note of any and all issues during the deployment.  Adjust your feature-specific deployment manifest and deployment plans accordingly.</li>
<li>Apex tests should pass in ALL environments.  Use continuous integration or schedule all tests to execute on a nightly basis (in each environment).  Waiting to deal with automated testing and code coverage until near the actual production migration is a recipe for disaster.</li>
<li>As the features migrate &#8220;upstream&#8221; through your test environments toward your production org, the feature-specific deployment plans and deployment manifests should be combined by the environment manager into the release-specific deployment package.  The environment manager should be responsible for migrating the entire deployment package through your test and staging environments.</li>
<li>As the features migrate &#8220;upstream&#8221; through the test environments towards your production org, the feature-specific release notes should be consolidated by the release manager.</li>
<li>If you would like to automate the ENTIRE release, including manual steps, create a deployment script (or set of scripts) using a web browser testing tool like Selenium.</li>
<li>Complete a mock-deployment into your staging environment.  This should be repeated as necessary to fully ensure that the deployment plan and deployment manifest are comprehensive.  This staging environment should be an EXACT replica of your production org.  Refresh a new sandbox if necessary to ensure the environments are identical.</li>
<li>Validate your final mock-deployment is successful via smoke-testing and simulated transactions.</li>
</ol>
</li>
<li>
<h4>Execute</h4>
<ol>
<li>Validate that the necessary approvals are obtained by both the business and IT stakeholders as per your Go/No-Go Governance methodology.</li>
<li>Start your change window, including notifying any appropriate parties.</li>
<li>Freeze all changes to production.  If necessary, lock-out any delegated admins to ensure your org metadata will NOT be changing without your knowledge.</li>
<li>Lock users out of the system.  This can be done by changing their profiles or temporarily freezing them.  Use data loader tools or Apex scripts to automate this process.  (Just don&#8217;t lock yourself out!)</li>
<li>Validate your deployment package in production, including Running All Tests.</li>
<li>Backup Salesforce Metadata.  This can be done by creating a new sandbox (or refreshing an old one).  This can also be done using the Metadata API and a source control tool.  Regardless of your approach, this provides you with a safety net to fall back on in case of issues.</li>
<li>Back up Salesforce data.  Depending on the scope and impact of the release, it might be appropriate to back up all data.  This can be done on a scheduled basis by Salesforce (once per week) or a nightly batch process completed by your own data loading tool. Depending on how long this process takes, it might be impractical to do this during your change window.  However try to have a backup completed as close to your change window as possible.</li>
<li>Complete any necessary &#8220;pre-migration tasks&#8221; outlined in your deployment plan.  These will be any manual steps necessary to &#8220;receive&#8221; your deployment package.  (These can even be automated with a web-browser testing tool like Selenium.)</li>
<li>Deploy your deployment package to production using your deployment tool of choice.</li>
<li>Complete any necessary &#8220;post-migration tasks&#8221; outlined in your deployment plan.  These will be any manual steps necessary to provision the release that are NOT supported by the Metadata API.  (These can even be automated with a web-browser testing tool like Selenium.)</li>
<li>Repeat steps 8-10 as necessary to deploy your entire release as per your deployment plan.</li>
<li>Complete a metadata comparison between your &#8220;from&#8221; environment and your production org.  Make sure any differences are clearly understood or addressed.  You can use a tool like <a href="http://www.scootersoftware.com/" target="_blank">Beyond Compare</a>, <a href="http://www.grigsoft.com/download-windiff.htm" target="_blank">WinDiff</a>, or even custom Perl/Python scripts.</li>
<li>Load Data.  Once the metadata is stable, any necessary data conversions or integrations can be activated.  Closely monitor the initial transactions to ensure system integrity.</li>
<li>Smoke test the functionality in production.  Regardless of the technical discipline used on the deployment, manually validate that the deployment was successful (so your users won&#8217;t be the first ones to do so).</li>
<li>Re-extract all metadata and tag your release in source control.  A release becomes an important milestone to mark your code should you ever need to fall back to an old version.</li>
<li>Unlock users, allowing them back into the system. (Automatically with data loader tools or Apex scripts.)</li>
<li>Close your change window, including notifying any appropriate parties.</li>
<li>Send a notification of the release to the appropriate users and stakeholders as per your communication plan.</li>
<li>Distribute your release notes to any appropriate users.</li>
<li>Update your configuration workbook documentation to indicate the new features are &#8220;deployed.&#8221;</li>
<li>Refresh any necessary sandboxes and/or make your deployed package available to your sandboxes.</li>
</ol>
</li>
</ol>
<p>As you can see &#8211; there are a lot of people, tools, and processes necessary for supporting a smooth roll-out.  Plan accordingly, rehearse your plan, and follow this criteria in order to ensure your enterprise deployments become predictable and flawless.  (And if the above list is not enough to keep you busy, make sure you have a good plan for training and support.  All of these functions are just as critical to a good deployment and user adoption.)</p>
<p>For more help on executing flawless deployments, consider bringing on a Salesforce <a title="Salesforce Certified Architect" href="https://developer.salesforce.com/page/Certification">Certified Technical Architect (CTA)</a>.  CTAs have the technical platform knowledge combined with Application Lifecycle Management experience to help your organization get started.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/SforceBlog?a=sXxZv4f5LWQ:-z4RHmg5WUw:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=sXxZv4f5LWQ:-z4RHmg5WUw:qj6IDK7rITs"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=qj6IDK7rITs" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=sXxZv4f5LWQ:-z4RHmg5WUw:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=sXxZv4f5LWQ:-z4RHmg5WUw:V_sGLiPBpWU" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=sXxZv4f5LWQ:-z4RHmg5WUw:F7zBnMyn0Lo"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=sXxZv4f5LWQ:-z4RHmg5WUw:F7zBnMyn0Lo" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=sXxZv4f5LWQ:-z4RHmg5WUw:l6gmwiTKsz0"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=l6gmwiTKsz0" border="0"></img></a>
</div><img src="//feeds.feedburner.com/~r/SforceBlog/~4/sXxZv4f5LWQ" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://developer.salesforce.com/blogs/developer-relations/2014/12/salesforce1-enterprise-deployment-strategy.html/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		<feedburner:origLink>http://developer.salesforce.com/blogs/developer-relations/2014/12/salesforce1-enterprise-deployment-strategy.html</feedburner:origLink></item>
		<item>
		<title>Ringing the Changes in the Force.com JavaScript REST Toolkit</title>
		<link>http://feedproxy.google.com/~r/SforceBlog/~3/-eXUDoScoi8/ringing-changes-force-com-javascript-rest-toolkit.html</link>
		<comments>http://developer.salesforce.com/blogs/developer-relations/2014/12/ringing-changes-force-com-javascript-rest-toolkit.html#comments</comments>
		<pubDate>Fri, 12 Dec 2014 01:09:06 +0000</pubDate>
		<dc:creator><![CDATA[Pat Patterson]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://26.92261</guid>
		<description><![CDATA[<a href="http://developer.salesforce.com/blogs/developer-relations/2014/12/ringing-changes-force-com-javascript-rest-toolkit.html"><img align="right" hspace="5" width="150" src="http://res.cloudinary.com/hzxejch6p/image/upload/c_scale,w_250/v1418346012/CreateBlob_ua5cdg.png" class="alignright wp-post-image tfe" alt="" title="" /></a>The Force.com JavaScript REST Toolkit ('ForceTK' for short) is a minimal JavaScript wrapper for the Force.com REST API. This blog entry looks at recent changes that bring it up to date with the evolution of the Salesforce1 Platform.]]></description>
				<content:encoded><![CDATA[<p><img class="alignleft wp-image-92291" alt="" src="http://res.cloudinary.com/hzxejch6p/image/upload/c_scale,w_250/v1418346012/CreateBlob_ua5cdg.png" width="250" height="131" />Although there are now <a href="https://developer.salesforce.com/blogs/developer-relations/2013/03/using-javascript-with-force-com.html">many, many ways</a> to interact with Force.com from JavaScript, the <a href="https://github.com/developerforce/Force.com-JavaScript-REST-Toolkit">Force.com JavaScript REST Toolkit</a> (or &#8216;ForceTK&#8217;), <a href="https://developer.salesforce.com/blogs/developer-relations/2011/03/calling-the-rest-api-from-javascript-in-visualforce-pages.html">first released in March 2011</a>, was one of the pioneers, and remains one of the most popular. ForceTK provides a fairly minimal JavaScript wrapper for the Force.com REST API, with methods to create, retrieve, update and delete records, freeing the developer from the chore of constructing resource URLs and making HTTP calls.</p>
<p>The Salesforce1 Platform (comprising Force.com, its REST API and much more) is anything but static, however, and ForceTK has just undergone a minor refit to bring it up to date. Let&#8217;s take a look at three changes in the current version.</p>
<p>One of the main benefits of ForceTK is that it provides the same interface for accessing the Force.com REST API from JavaScript running in Visualforce, on external servers, and in hybrid mobile apps. Part of this abstraction is routing calls via a proxy where the browser&#8217;s <a href="http://en.wikipedia.org/wiki/Same-origin_policy">Same Origin Policy</a> prohibits direct calls to the Force.com REST API endpoint. From Spring &#8217;13 on, the endpoint (<code>/services/data</code>) is exposed on the Visualforce servers, so the proxy is not required. The library will automatically call the REST API directly in this case. Note that Apex REST methods (<code>/services/apexrest</code>) still require the proxy.</p>
<p>A downside of using the REST API is that <a href="https://help.salesforce.com/HTViewHelpDoc?id=integrate_api_rate_limiting.htm">API calls are a limited resource</a>. <a href="https://developer.salesforce.com/blogs/developer-relations/2012/10/not-calling-the-rest-api-from-javascript-in-visualforce-pages.html">RemoteTK</a> was written to allow JavaScript on Visualforce pages to avoid consuming API calls by using JavaScript Remoting, but came with a trade off &#8211; since calls were routed via an Apex controller, they operated with system-level privileges. <a href="https://www.salesforce.com/us/developer/docs/pages/index_Left.htm#CSHID=pages_remote_objects.htm|StartTopic=Content%2Fpages_remote_objects.htm|SkinName=webhelp">Visualforce Remote Objects</a>, released in Winter &#8217;15, provide a much better mechanism, since they correctly respect user-level sharing rules and field level security. RemoteTK has been removed from the toolkit, and developers should migrate their code to Visualforce Remote Objects.</p>
<p>One final addition is the ability to <a href="https://developer.salesforce.com/docs/atlas.en-us.api_rest.meta/api_rest/dome_sobject_insert_update_blob.htm">insert and update Blob data via multipart messages</a> &#8211; a REST API feature currently available through a pilot program. It is possible to insert or update blob data using a non-multipart message (by passing base64-encoded data in a JSON or XML-encoded message), but if you do, you are limited to 50 MB of text data or 37.5 MB of base64–encoded data. Using multipart messages, you can upload files of any type with a size of up to 500 MB. The new <code>createBlob</code> and <code>updateBlob</code> functions allow creation of <code>ContentVersion</code> records and creation and update of <code>Document</code> records. Here&#8217;s a minimal example that shows how it works:</p>
<pre>&lt;apex:page&gt;
    &lt;script src="{!$Resource.forcetk_new}"&gt;&lt;/script&gt;
    &lt;p&gt;
        Select a file to upload as a new Chatter File.
    &lt;/p&gt;
    &lt;input type="file" id="file" onchange="upload()"/&gt;
    &lt;p id="message"&gt;&lt;/p&gt;
    &lt;script&gt;
        var client = new forcetk.Client();
        client.setSessionToken('{!$Api.Session_ID}');

        function upload() {
            var file = document.getElementById("file").files[0];
            client.createBlob('ContentVersion', {
                Origin: 'H', // 'H' for Chatter File, 'C' for Content Document
                PathOnClient: file.name
            }, file.name, 'VersionData', file, function(response){
                console.log(response);
                document.getElementById("message").innerHTML = 
                    "Chatter File created: &lt;a href="/" + response.id + 
                    ""&gt;Take a look!&lt;/a&gt;";
            }, function(request, status, response){
                console.log(response);
                document.getElementById("message").innerHTML = 
                    "Error: " + status;
            });
        }
    &lt;/script&gt;
&lt;/apex:page&gt;</pre>
<p>If you&#8217;re using ForceTK, grab the new version and let me know what you think in the comments.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/SforceBlog?a=-eXUDoScoi8:mceIsBUdohY:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=-eXUDoScoi8:mceIsBUdohY:qj6IDK7rITs"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=qj6IDK7rITs" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=-eXUDoScoi8:mceIsBUdohY:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=-eXUDoScoi8:mceIsBUdohY:V_sGLiPBpWU" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=-eXUDoScoi8:mceIsBUdohY:F7zBnMyn0Lo"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=-eXUDoScoi8:mceIsBUdohY:F7zBnMyn0Lo" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=-eXUDoScoi8:mceIsBUdohY:l6gmwiTKsz0"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=l6gmwiTKsz0" border="0"></img></a>
</div><img src="//feeds.feedburner.com/~r/SforceBlog/~4/-eXUDoScoi8" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://developer.salesforce.com/blogs/developer-relations/2014/12/ringing-changes-force-com-javascript-rest-toolkit.html/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		<feedburner:origLink>http://developer.salesforce.com/blogs/developer-relations/2014/12/ringing-changes-force-com-javascript-rest-toolkit.html</feedburner:origLink></item>
		<item>
		<title>Salesforce1 Platform Enterprise Environment Management</title>
		<link>http://feedproxy.google.com/~r/SforceBlog/~3/mXi6qCwCAiw/salesforce1-enterprise-environment-management.html</link>
		<comments>http://developer.salesforce.com/blogs/developer-relations/2014/12/salesforce1-enterprise-environment-management.html#comments</comments>
		<pubDate>Tue, 09 Dec 2014 17:16:55 +0000</pubDate>
		<dc:creator><![CDATA[Greg Cook]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Enterprise Architecture]]></category>
		<category><![CDATA[Environment Management]]></category>

		<guid isPermaLink="false">http://26.91871</guid>
		<description><![CDATA[Establishing an effective Environment Management strategy is key for utilizing the Salesforce1 platform.  Here is an environment management strategy that can be emulated on many Enterprise projects.]]></description>
				<content:encoded><![CDATA[<p><em>Editors Note: This post is part of a “Guest” series entitled Enterprise Architecture with Force.com. Our <em>guest blogger, </em>Greg Cook, is a managing partner of CloudPremise and currently holds all seven Salesforce certifications.</em></p>
<p><img class="alignleft wp-image-92531" alt="" src="https://res.cloudinary.com/hzxejch6p/image/upload/v1418669920/Greg_Cookx150_m1ke1a.jpg" width="122" height="150" />Establishing an effective Environment Management strategy is critical for utilizing the Salesforce1 Platform.  Salesforce.com has already established itself as a leader in cloud technology and innovation, however some aspects of dealing with the platform still require good-old-fashioned IT management skills.  Salesforce provides a LOT of content around governance, environment management, and change management.  However I have noticed at multiple clients the information and options are sometimes too plentiful.  Many customers want to be given a recipe and a framework as opposed to designing their own solutions from best practices.  This installment describes a typical environment management strategy that can be emulated on many Enterprise projects.</p>
<p>What do I mean by Enterprise Projects?  This article applies to your organization if you meet the following criteria:</p>
<ol>
<li>Your Salesforce landscape is made up of one or more orgs consisting of &#8220;high complexity&#8221; (lots of configuration, lots of code, lots of data).</li>
<li>Your Salesforce projects are required to follow a formal change process under the governance of your Enterprise Architecture and/or ITSM frameworks.</li>
<li>Your Salesforce projects consist of many resources including architects, business analysts, configurators, developers, and testers.</li>
<li>You have have a large number of requirements often prioritized by competing business stakeholders.</li>
<li>You have active system administrators who are making authorized (and sometimes unauthorized) changes directly in production.</li>
<li>You constantly have issues deploying changes to production.</li>
</ol>
<p>If this sounds familiar than you are in luck and this article was written for you.  If your projects do not sound like the above list then many of the recommendations in this article may not apply to your environment (but feel free to keep reading!)</p>
<h2>What You Should Know About Environment Management</h2>
<p>The first thing to understand about environment management on the Salesforce1 Platform are some of the differences to typical environment management.</p>
<h2>1. Your Salesforce Production Org is Your Only &#8220;Pristine&#8221; Environment</h2>
<p>In traditional software development your pristine environment can be housed within source control and configuration management tools. In order to maintain a stable production deployment the emphasis is using configuration management techniques to maintain your code, configuration files, and deployment scripts.  Production can be rebuilt, deployed at will, and rolling back to a previous version of the code is possible.  However in Salesforce it is not possible to take production offline or to deploy the entire application in a big-bang deployment event.  Instead you are trying to migrate &#8220;differences&#8221; between your environments into production.  You can utilize source control and configuration management tools with Salesforce, however your production environment is (almost) always live regardless of the state of your configuration management procedures.</p>
<h2>2. Salesforce is a heavily configured environment &#8211; much of which can be done directly in production</h2>
<p>Salesforce supports a much higher volume of &#8220;production changes&#8221; due to its metadata based configuration design.  Therefore many changes can be done safely in production without the need for a code deployment.  While this has the effect of producing immediate value to the business (Admin Hero&#8217;s anyone?), it can have dreadful consequences on future deployments if the right controls are not in place.</p>
<h2>3. The larger the difference between environments directly correlates to the difficulty in migrating between those environments</h2>
<p>As I already described, it is not possible to move the entire code base from one state to another (i.e. Dev &#8211;&gt; QA &#8211;&gt; Prod, etc).  The nature of Salesforce&#8217;s migrations means that the emphasis should not be on your configuration management artifacts, but rather the emphasis should be on <strong><em>environment synchronization</em></strong>.  You may have the best code and most thorough migration plan &#8211; but if your environments are out of sync with production changes you will have a VERY difficult time deploying changes of significant complexity.</p>
<h2>4. Your sub-prod environments differ from your Production environment</h2>
<p>This is NOT uncommon on traditional projects.  However there are specific pain points around Salesforce sandboxes.  The most noticeable difference will be the database size.  Many customers do not even have a full copy sandbox, let alone multiple.  Most sandboxes will not come loaded with data, and those that do may need changes to support testing and integration.  There are other differences outside the scope of this article, however the main pain point remains: there will be overhead in each sub-prod environment to manage data and metadata.  This overhead increases as the number of sandboxes increase.</p>
<h2>5. Not all changes to Salesforce can be automated via the API</h2>
<p>Salesforce is releasing significant new features three times per year.  While many of these new features can be managed and maintained using the APIs, some changes are not possible except through the web-browser interface.  That means that maintaining a strict documentation set is <strong>VITAL</strong> for successful migration and environment maintenance.</p>
<p>With those prerequisites out of the way it is time to introduce my reference model for Enterprise Environment Management:</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><a href="https://gregorydcook.files.wordpress.com/2014/11/environment-management1.png"><img alt="Environment Management" src="https://gregorydcook.files.wordpress.com/2014/11/environment-management1.png" width="639" height="460" /></a></p>
<p>Let&#8217;s walk through the diagram and explain the concepts and purpose of each component, including the types of change to each environment.</p>
<h3>#1 &#8211; Production</h3>
<p>As mentioned, this is your only pristine environment.  Refreshes are only available from production into the sandboxes (reflected by the red lines).  Therefore your code and configuration changes are &#8220;swimming upstream&#8221; &#8211; trying to migrate successfully to the production environment from the lower sub-prod environments.</p>
<ul>
<li><span style="color: #0000ff;">API Based Deployments </span>- Your production deployments should take place mostly through a strictly controlled process via ANT or Change sets.  The only environments I would allow to migrate into Production would be the System Test (#6), Stage (#7), or PFix (#9) environments.</li>
<li><strong>Manual Changes (Configuration and Data)</strong> &#8211; Any configuration changes made to production must be applied to sub-prod environments either manually or via a refresh.  Subsets of production data must also be pushed into multiple sandboxes to support testing, training, and integration.</li>
<li><span style="color: #ff0000;">Refreshes</span> - You can and should refresh sub-prod environments often.  If you do NOT refresh your sub-prod environments you have 2 choices: 1) manually maintain ALL changes from Production to each &#8220;managed&#8221; sub-prod environment or 2) risk very difficult and unpredictable deployments.</li>
</ul>
<h3>#2 &#8211; Un-managed Developer Environments</h3>
<p>Each developer on the project will need an environment.  Typically these environments are Developer Sandboxes.  The care and feeding of this environment is usually done by the developer themselves.  These environments typically only exist for the length of time it takes to migrate a feature to production, and sometimes even less.  I call these &#8220;unmanaged&#8221; environments because typically they will not be maintained by your environment manager.  A developer should complete <a href="http://en.wikipedia.org/wiki/Unit_testing" target="_blank">Unit testing</a> and apex-based automated testing in this environment.</p>
<ul>
<li><span style="color: #ff0000;">Refreshes</span> - I typically expect these sandboxes to be refreshed after a working feature has been successfully migrated upstream.</li>
<li><span style="color: #339966;">Manual/API Maintenance</span> - Changes from other developers (and eventually other projects) must be successfully applied to each developer sandbox.  This &#8220;merging&#8221; of configuration and code can be done multiple ways including source code tools, change sets/metadata API, or manually.  This step is very important depending on the number of developers and the number of parallel projects inside a single org.</li>
<li><span style="color: #0000ff;">API Based Deployments</span> - Working features should be deployed upstream using Change Sets or ANT.  Get in the habit of maintaining a strict manifest (list, index, catalog, etc) of components that need to be migrated.</li>
</ul>
<h3>#3 &#8211; Managed Project Environments</h3>
<p>Each project needs to have an environment that can isolate changes from other projects.  Many projects may be occurring at the same time and on different release schedules.  These managed environments are the holding grounds for project features that are not ready for release.  This is where I would have testers complete <a href="http://en.wikipedia.org/wiki/Functional_testing" target="_blank">Functional </a>testing.  I call these &#8220;managed&#8221; environments because typically they are (or should) be maintained by an environment manager (a system administrator with responsibility for environment integrity).</p>
<ul>
<li><span style="color: #ff0000;">Refreshes</span> - Typically these sandboxes would only be refreshed after a project has migrated upstream.  Sometimes it is necessary to refresh a sandbox prior to this upstream migration, in which case all project changes would have to be reapplied.  Source control tools are great for this; however expect manual maintenance as well.</li>
<li><strong>Manual Changes (Configuration and Data)</strong> &#8211; Any configuration changes made to production must be applied to sub-prod environments either manually or via a refresh.  Sub-sets of data must also be pushed into sandboxes to support testing and integration.</li>
<li><span style="color: #339966;">Manual/API Maintenance</span> - Changes from other projects must be successfully applied to each project sandbox.  This &#8220;merging&#8221; of configuration and code can be done multiple ways including source code tools, change sets/metadata API, or manually.  This step is very important depending on the number of parallel projects inside a single org.  This step becomes more or less important based upon the release calendar of each project, and whether the project sandbox can afford being refreshed mid-stream.</li>
<li><span style="color: #0000ff;">API Based Deployments</span> - When a project is ready to move into production it should be migrated upstream into the &#8220;release train&#8221;.  Changes should be migrated using Change Sets or ANT.  Get in the habit of maintaining a strict manifest (list, index, catalog, etc) of components that need to be migrated.</li>
</ul>
<p>Note: I have included Citizen Developer Environments in this region.  Citizen Developers would be small projects that do not require multiple developers/sandboxes.  However it is important to have the Citizen Developers migrate into production using the same path as your enterprise projects (i.e. the release train).</p>
<h3>#4 &#8211; Managed Release Train</h3>
<p>I use the concept of a release train to describe the activities necessary to migrate successfully to production.  A release train can be a simple schedule detailing when changes can be made to each environment, or it can be a complex automated solution.  You can read about release trains more <a href="http://scaledagileframework.com/agile-release-train/" target="_blank">here</a>.  For the purpose of this article I will use a very simple example of a release train:</p>
<ul>
<li>Only changes bound for production should be on the train.  Therefore make sure your unit and functional testing in the lower environments is very thorough.</li>
<li>Only on a predefined basis is movement allowed from one &#8220;car&#8221; to the next.  For example, a weekly cycle (i.e. Tuesday nights, etc) would be change window in which migration could occur on the train.  The cadence would be set based upon your organization&#8217;s capacity to support deployment activities.  It is important to set a precedence on release train migrations of &#8220;often enough but not too often&#8221; as well as &#8220;highly disciplined and predictable&#8221;.</li>
<li>The number of environments inside your release train will be determined by the types of gates necessary to support your deployment process.  I have recommended three stops (integration, test, stage) but this maybe more or less depending on your specific organization.</li>
</ul>
<p>Let&#8217;s look deeper at each stop on the release train:</p>
<h3>#5 &#8211; System Integration</h3>
<p>This is the environment where parallel projects comes together.  This is also a good spot for <a href="http://en.wikipedia.org/wiki/System_integration_testing" target="_blank">system integration testing</a>.  Automated apex tests should be continually run in this environment to ensure a project has not broken something.  Consider the use of more sophisticated automation tools to ensure your business processes are protected.  <a href="http://www.seleniumhq.org/" target="_blank">Selenium </a>is a good open source tool for testing the user interface.  You can also test APIs and integrations with tools like <a href="http://www.soapui.org/" target="_blank">SoapUI </a>or <a href="https://chrome.google.com/webstore/detail/postman-rest-client/fdmmgilgnpjigdojojpjoooidkmcomcm?hl=en" target="_blank">Postman</a>.</p>
<ul>
<li><span style="color: #ff0000;">Refreshes</span> - I would refresh the system integration sandbox as often as is practical.  Therefore I would NOT recommend a full-sandbox with its accompanying 29 day refresh limit.</li>
<li><strong>Manual Changes (Configuration and Data)</strong> &#8211; If you will not refresh your sandbox then expect a much higher level of manual maintenance.  All production changes must be pushed into this environment one way or another.</li>
<li><span style="color: #339966;">Manual/API Maintenance</span> - If necessary code can be pushed from System Integration &#8220;back down&#8221; into a lower sub-prod environments.  This would allow parallel projects to integrate sooner.  This is a great technique if you know there will be conflict between two projects (i.e. both working on the same trigger).</li>
<li><span style="color: #0000ff;">API Based Deployments</span> - When code has successfully passed the necessary gates it can be migrated to the next environment on the release train.  Use change sets or ANT as well as detailed documentation.</li>
</ul>
<h3>#6 &#8211; System Test</h3>
<p>This environment should be reflective of your desired production state. This is where I would recommend <a href="http://en.wikipedia.org/wiki/Acceptance_testing" target="_blank">acceptance testing</a>.  You should be able to conduct end-to-end business processes in this environment.  I typically would use this environment for my Full Copy sandbox as it will change less often than other environments.  Load testing and performance testing can also be validated in this environment due to the larger database size.</p>
<ul>
<li><span style="color: #ff0000;">Refreshes</span> - I would refresh the system test sandbox as often as is practical.  Ideally you would refresh this every 29 days.</li>
<li><strong>Manual Changes (Configuration and Data)</strong> &#8211; If you will not refresh your sandbox then expect a much higher level of manual maintenance.  All production changes must be pushed into this environment one way or another.  Data that comes into the full copy sandbox may need to be changed to support testing, integration, or even privacy requirements.  Tooling and automated scripts come in handy here.</li>
<li><span style="color: #0000ff;">API Based Deployments</span> - When code has successfully passed the necessary gates it is ready to move to production.  But not QUITE ready&#8230; you must first complete a staged deployment.</li>
</ul>
<h3>#7 - Stage</h3>
<p>This environment&#8217;s sole purpose is to ensure your deployments to production will succeed.  I would definitely not use a full sandbox here.  In fact I would refresh this environment prior to EACH production deployment.  That way you know you have the most up to date configuration from production.  I hope you followed my advice and used change sets or ANT throughout the migrations.  That means you have a comprehensive deployment manifest.  I also hope you followed my advice and kept detailed documentation for all manual changes.  That means you have a comprehensive set of deployment instructions.  After deploying to stage you should conduct <a href="http://en.wikipedia.org/wiki/Smoke_testing_(software)" target="_blank">smoke testing</a> to ensure your deployment was successful.</p>
<ul>
<li><span style="color: #ff0000;">Refreshes</span> - Refresh often.  Enough said.</li>
<li><strong>Manual Changes (Configuration and Data)</strong> - Theoretically you would not need to maintain any manual changes to this environment as they would all be brought over via the refresh.  However some manual steps to provision the environment will still be necessary, especially data related to any smoke testing.</li>
<li><span style="color: #0000ff;">API Based Deployments</span> - You can choose whether you want to deploy to Production from Stage or System Test.  However I am recommending only using Stage for practicing your deployment, in which case you could tear down the environment immediately upon validating your mock deployment.</li>
</ul>
<p>If your deployment was successful then you have a very high assurance that your production deployment will also be successful.  Repeat the steps used in the stage deployment during the production deployment.</p>
<h3>#8 - Training Environment(s)</h3>
<p>Training is a difficult issue due to the nature of environment setup and tear down.  It is also difficult to determine whether to have the code that is ABOUT to go live versus the code that HAS ALREADY gone live.  So you have a few choices.  You can decide on one, multiple, or all of the following:</p>
<ol>
<li>Migrate from System Test into a &#8220;Pre-Release Training Environment&#8221;.  Similar to a stage deployment but this environment would be persisted and will need a much more thourough data set to support training.</li>
<li>Refresh from Production into a &#8220;Post-Release Training Environment&#8221;.  This will ensure you have the latest and greatest metadata to support training.  You will still need to maintain data in this environment.</li>
<li>Train directly in production.  Use mock data that is recognized (and therefore ignored) enterprise wide.  This sounds scary but I&#8217;ve seen it work very well.  It also lowers your environment management costs.</li>
<li>Create training applications or videos.  You can create training videos or even interactive HTML5 applications that allow users to observe small business processes.  This can often be done much cheaper than maintaining live training environments.</li>
</ol>
<h3>#9 - Pfix (Production Fix)</h3>
<p>No matter what you do, someday a Sev 1 bug will reveal itself.  So where should you make the fix and how do you get it deployed to production as quickly as possible?  My recommendation in these situations is to generate a new sandbox only to respond to the Sev 1 defect.  The configuration or code can be fixed, immediately tested, and immediately deployed to Production.  This should ONLY be done under dire circumstances as you are bypassing many of the controls I have outlined in this article.  Just make sure to reapply the fix into your sub-prod environments as necessary.</p>
<h3>Enough Already!</h3>
<p>As you can see there is quite a bit of activity that needs to take place in order to orchestrate a pristine environment plan.  And this article only covers migration actives, not even production support issues like data backup, archive, etc.  I hope it is clear by now that I HIGHLY recommend hiring a full-time Salesforce.com Environment Manager to plan, execute, and monitor all of the items outlined above.  An Environment Manger would be similar to a System Administrator &#8211; however their focus is very different.  A Salesforce system administrator is typically focused on users.  An Environment Manager would only be focused on the technical infrastructure.</p>
<p>When consulting with enterprise customers I often encounter issues like &#8220;how to maintain sandboxes&#8221; and &#8220;how to ensure smooth production deployments&#8221;.  And typically none of these customers have a dedicated Environment Manager who is following a robust strategy and executing these detailed tactics.  An investment in a dedicated Environment Manager will allow your company to scale Salesforce1 must more effectively.</p>
<h2>Summary</h2>
<p>Environment management is one of the most difficult and under-realized aspects on enterprise projects.  Companies often under estimate the amount of work necessary to orchestrate the movement of data and metadata throughout the environments.  The right strategy, the correct resources (human and technical), and effective processes can radically accelerate and improve your consumption of the Salesforce1 Platform.  You can harness the <strong>agility</strong> of Salesforce with the <strong>predictability</strong> of enterprise class deployments.  If you are looking for help on getting started, consider obtaining the help of a <a title="Salesforce Certification" href="https://developer.salesforce.com/page/Certification">Salesforce.com Certified Technical Architect</a> to help your company define and execute upon an Environment Management strategy.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/SforceBlog?a=mXi6qCwCAiw:774r8fJ--1o:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=mXi6qCwCAiw:774r8fJ--1o:qj6IDK7rITs"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=qj6IDK7rITs" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=mXi6qCwCAiw:774r8fJ--1o:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=mXi6qCwCAiw:774r8fJ--1o:V_sGLiPBpWU" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=mXi6qCwCAiw:774r8fJ--1o:F7zBnMyn0Lo"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=mXi6qCwCAiw:774r8fJ--1o:F7zBnMyn0Lo" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=mXi6qCwCAiw:774r8fJ--1o:l6gmwiTKsz0"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=l6gmwiTKsz0" border="0"></img></a>
</div><img src="//feeds.feedburner.com/~r/SforceBlog/~4/mXi6qCwCAiw" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://developer.salesforce.com/blogs/developer-relations/2014/12/salesforce1-enterprise-environment-management.html/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		<feedburner:origLink>http://developer.salesforce.com/blogs/developer-relations/2014/12/salesforce1-enterprise-environment-management.html</feedburner:origLink></item>
		<item>
		<title>WSDL 2 Apex Winner!</title>
		<link>http://feedproxy.google.com/~r/SforceBlog/~3/gD84k8jwhzk/wsdl-2-apex-winner.html</link>
		<comments>http://developer.salesforce.com/blogs/engineering/2014/12/wsdl-2-apex-winner.html#comments</comments>
		<pubDate>Mon, 08 Dec 2014 21:57:41 +0000</pubDate>
		<dc:creator><![CDATA[Josh Kaplan]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://25.33531</guid>
		<description><![CDATA[I am (finally) excited to announce the winner of our $100 Hackathon contest!  As you all recall, we released the WSDL 2 Apex project as open-source in September. In that announcement, I challenged you to put on your coding gloves and box the heck out of those pesky frustrations you had with the current WSDL 2 Apex generator. <br /><br /><b>Daniel Ballinger</b>, better known as "FishOfPrey", punched out not one, not two, but three issues in a short amount of time. He is your $100 Hackathon Champion!<br /><br />.]]></description>
				<content:encoded><![CDATA[<p>I am (finally) excited to announce the winner of our $100 Hackathon contest!  As you all recall, we released the WSDL 2 Apex project as open-source in September. If you don&#8217;t recall, you can recall by reading my previous blog post on the topic: <a href="https://developer.salesforce.com/blogs/engineering/2014/09/announcing-open-source-wsdl2apex-generator.html">My Previous Blog Post On The Topic</a></p>
<p>In that announcement, I challenged you to put on your coding gloves and box the heck out of those pesky frustrations you had with the current WSDL 2 Apex generator. <strong>Daniel Ballinger</strong>, better known as <span style="text-decoration: underline">&#8220;FishOfPrey&#8221;</span>, punched out not one, not two, but three issues in a short amount of time. He is your $100 Hackathon Champion!</p>
<p>What did our FishOfPrey friend fix? I&#8217;m glad you asked! The main thing that he fixed was adding integer and boolean as reserved keywords, such that variables of these types get the _x suffix like their primitive colleagues. More consistent output for you and your WSDL generations!  He also fixed an issue with different delimiters in WSDL files created in Windows, and removed the size limitation on the WSDL file.  Yes, that is right; the open source WSDL generator does not have all of the limitations of the multi-tenant online version. Hooray, open source!</p>
<p>Here is my interview with Daniel, which did not actually take place:</p>
<p style="padding-left: 30px"><span style="text-decoration: underline">Josh</span>: Congratulations on winning!</p>
<p style="padding-left: 30px"><span style="text-decoration: underline">FishOfPrey</span>: Thanks!</p>
<p style="padding-left: 30px"><span style="text-decoration: underline">Josh</span>: What is a &#8220;fish of prey&#8221;? Is that the predator who chases the prey? Or the fish running away from said predator?</p>
<p style="padding-left: 30px"><span style="text-decoration: underline">FishOfPrey</span>: Well, it depends on&#8230;</p>
<p style="padding-left: 30px"><span style="text-decoration: underline">Josh (interrupting)</span>: I mean, are you a shark? A barracuda? A minnow? I must know.</p>
<p style="padding-left: 30px"><span style="text-decoration: underline">FishOfPrey</span>: I suppose barracuda? Could be either the predator or the prey, depending on the day? What does this have to do with code?</p>
<p style="padding-left: 30px"><span style="text-decoration: underline">Josh</span>: Nothing, really. But seriously, you did a bang-up job! Can people send you gifts?</p>
<p style="padding-left: 30px"><span style="text-decoration: underline">FishOfPrey</span>: Yes, so long as they are not sending sharks or other large ocean predators.</p>
<p>As you can see, Daniel and I are challenging you again. Who among you is a shark? Who can tackle the big open items in the WSDL 2 Apex generator? A shark does not need a hackathon! Mark Cuban has a whole tank of sharks on TV, and they don&#8217;t need hackathons! If you get stuck while doing Apex class generation, shark-up™ and fix the problem. You will get help from the team and the community; we all want to make this code project as robust as possible.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/SforceBlog?a=gD84k8jwhzk:r8YjJLtY_NM:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=gD84k8jwhzk:r8YjJLtY_NM:qj6IDK7rITs"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=qj6IDK7rITs" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=gD84k8jwhzk:r8YjJLtY_NM:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=gD84k8jwhzk:r8YjJLtY_NM:V_sGLiPBpWU" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=gD84k8jwhzk:r8YjJLtY_NM:F7zBnMyn0Lo"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=gD84k8jwhzk:r8YjJLtY_NM:F7zBnMyn0Lo" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=gD84k8jwhzk:r8YjJLtY_NM:l6gmwiTKsz0"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=l6gmwiTKsz0" border="0"></img></a>
</div><img src="//feeds.feedburner.com/~r/SforceBlog/~4/gD84k8jwhzk" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://developer.salesforce.com/blogs/engineering/2014/12/wsdl-2-apex-winner.html/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		<feedburner:origLink>http://developer.salesforce.com/blogs/engineering/2014/12/wsdl-2-apex-winner.html</feedburner:origLink></item>
		<item>
		<title>Agile and Innovation</title>
		<link>http://feedproxy.google.com/~r/SforceBlog/~3/-56gQNAazYU/pton-inspires-learning-innovation.html</link>
		<comments>http://developer.salesforce.com/blogs/engineering/2014/12/pton-inspires-learning-innovation.html#comments</comments>
		<pubDate>Wed, 03 Dec 2014 17:00:32 +0000</pubDate>
		<dc:creator><![CDATA[Anjali Joshi]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[agile]]></category>
		<category><![CDATA[culture]]></category>
		<category><![CDATA[innovation]]></category>
		<category><![CDATA[learning]]></category>
		<category><![CDATA[pton]]></category>
		<category><![CDATA[Scrum]]></category>

		<guid isPermaLink="false">http://25.33451</guid>
		<description><![CDATA[<a href="http://developer.salesforce.com/blogs/engineering/2014/12/pton-inspires-learning-innovation.html"><img align="right" hspace="5" width="150" src="http://res.cloudinary.com/hzxejch6p/image/upload/v1417549956/SF_PTONTitle_fwtxeq.png" class="alignright wp-post-image tfe" alt="" title="" /></a>We have a program in Salesforce Engineering called ‘PTOn’ in which employees are encouraged to take 1 day per month to do something different than their day-to-day work. Some people use their time to create new applications on our Platform; some people work to improve our automation processes; and, others spend time reading and learning a new technology. I chose to speak at the Scrum Alliance conference to get our Agile story out to a larger, global audience and to inspire learning and innovation. I wanted to learn from an Agile community of engineers in another country.]]></description>
				<content:encoded><![CDATA[<p><img class="alignnone wp-image-33461" alt="" src="http://res.cloudinary.com/hzxejch6p/image/upload/v1417549956/SF_PTONTitle_fwtxeq.png" width="1282" height="516" /></p>
<p>In 2009, I went to volunteer in the Favelas of Rio de Janiero, with the intention of changing the world! I may not have ‘changed the world’ in my months abroad, but what I did get was a different perspective that has been invaluable to me in my life and in my career. When I was presented with the opportunity to speak about Agile at Salesforce at the Scrum Alliance conference in Rio de Janiero, I was given another opportunity to gain a fresh perspective, this time in relation to Agile and Technology.</p>
<p>We have a program in Salesforce Engineering called ‘PTOn’ in which employees are encouraged to take 1 day per month to do something different than their day-to-day work. Some people use their time to create new applications on our Platform; some people work to improve our automation processes; and, others spend time reading and learning a new technology. I chose to speak at the Scrum Alliance conference to get our Agile story out to a larger, global audience and to inspire learning and innovation. I wanted to learn from an Agile community of engineers in another country.</p>
<p>The experience I had was better than I could have imagined. I was received with a warm welcome, people wanted to hear the Salesforce story and I presented to a full room, with standing room only. I was able to share our Agile journey and everything we learned that makes us who we are as a company. It felt great to give back by sharing our experience with the hopes that other companies could benefit. In answering the insightful questions that I was asked, I realized how much we use our own product to run our Agile processes. We use Chatter to collaborate and share; we use our Platform to build our in-house Agile tool and we test the limits of our own product as we use it.</p>
<p>In taking the time to do this presentation, not only was I able to share my knowledge, I gained a new sense of confidence challenging myself to do something new. I learned how our product helps customers create a more collaborative and transparent corporate culture, and how the Brazilian Government is using Agile to run their processes. I also met a team that is gamifying the Agile process with their software. The list is endless. Everything I learned, I am bringing back to my role as an Agile coach and I am better equipped to serve our company.</p>
<p>Innovation doesn’t just happen; you need to take the time to create space in which new ideas emerge. Nothing exciting comes out of doing the same thing day in and day out. When you step back from the daily routine of your life, you get the ability to see the bigger picture and look at things in a different way. It’s easier to solve problems and you are imbibed with a fresh dose of energy and passion to solve the problems you are facing.</p>
<p>I encourage you to take the time to take your own ‘PTOn’, taking at least 1 day per month to get out of your daily routine. Not only is it fun, but any time we put ourselves in a situation that challenges us and pushes us to grow, we develop in a way that better allows us to serve ourselves, our companies and our community.</p>
<p>I am grateful to work for a company that not only encourages, but expects and fosters an environment of innovation among its employees.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/SforceBlog?a=-56gQNAazYU:hjluKQXZGSI:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=-56gQNAazYU:hjluKQXZGSI:qj6IDK7rITs"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=qj6IDK7rITs" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=-56gQNAazYU:hjluKQXZGSI:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=-56gQNAazYU:hjluKQXZGSI:V_sGLiPBpWU" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=-56gQNAazYU:hjluKQXZGSI:F7zBnMyn0Lo"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=-56gQNAazYU:hjluKQXZGSI:F7zBnMyn0Lo" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=-56gQNAazYU:hjluKQXZGSI:l6gmwiTKsz0"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=l6gmwiTKsz0" border="0"></img></a>
</div><img src="//feeds.feedburner.com/~r/SforceBlog/~4/-56gQNAazYU" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://developer.salesforce.com/blogs/engineering/2014/12/pton-inspires-learning-innovation.html/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		<feedburner:origLink>http://developer.salesforce.com/blogs/engineering/2014/12/pton-inspires-learning-innovation.html</feedburner:origLink></item>
		<item>
		<title>Solving Java Memory Regressions with Zero Overhead and High Accuracy</title>
		<link>http://feedproxy.google.com/~r/SforceBlog/~3/p-IABIgItXs/solving-java-memory-regressions-high-accuracy-zero-overhead.html</link>
		<comments>http://developer.salesforce.com/blogs/engineering/2014/12/solving-java-memory-regressions-high-accuracy-zero-overhead.html#comments</comments>
		<pubDate>Tue, 02 Dec 2014 19:43:05 +0000</pubDate>
		<dc:creator><![CDATA[Oleg Gusak]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[heapdump]]></category>
		<category><![CDATA[java]]></category>
		<category><![CDATA[java heap]]></category>
		<category><![CDATA[JVM]]></category>
		<category><![CDATA[memory allocations]]></category>
		<category><![CDATA[Performance]]></category>

		<guid isPermaLink="false">http://25.29951</guid>
		<description><![CDATA[<a href="http://developer.salesforce.com/blogs/engineering/2014/12/solving-java-memory-regressions-high-accuracy-zero-overhead.html"><img align="right" hspace="5" width="150" src="http://res.cloudinary.com/hzxejch6p/image/upload/v1417559306/SF_JavaMemory-2_vdaek0.png" class="alignright wp-post-image tfe" alt="" title="" /></a>Identify the root cause of complex memory regressions by recording JVM object allocations with zero overhead, high accuracy, and existing tools.]]></description>
				<content:encoded><![CDATA[<p dir="ltr"><img class="alignnone wp-image-33491" alt="" src="http://res.cloudinary.com/hzxejch6p/image/upload/v1417559306/SF_JavaMemory-2_vdaek0.png" width="1282" height="516" /></p>
<p dir="ltr">Customer trust is Salesforce&#8217;s highest priority. Our customers trust us with their data and that our software platform will perform reliably. They also trust that our applications and architecture will be the fastest and most responsive user experience. That&#8217;s why performance is at the top of our priorities.</p>
<p>The Salesforce Performance Engineering team is tasked with ensuring that the platform and SaaS applications perform at the highest level.  Our team conducts extensive performance tests continually. We monitor and analyze the results and resolve any regressions that are found. Even a few percentage points degradation in performance is not allowed to go into production.</p>
<p dir="ltr">The performance testing is done in the form of workloads. A workload is a repeatable load test consisting of a set of user requests that exercise specific features or functionalities (Apex cache, Visualforce pages, or Chatter feeds for example). A given workload is run periodically, usually daily, on the latest code version at that time. We achieve repeatability and high accuracy of the test through full automation of the run, data collection, and data analysis.  The performance engineering team relies mostly on open source tools (e.g., <a href="http://jmeter.apache.org/">JMeter</a> for generating load) and tools developed in-house (e.g., test automation orchestration, data collection, and results processing).</p>
<p dir="ltr">The code of core application servers is built with performance in mind. It is extensively instrumented to provide various performance metrics that supply extremely valuable information, especially for monitoring production health and troubleshooting incidents. This information is recorded in the server logs and is collected and analyzed. Besides the server logs, our test automation collects and analyzes system performance metrics provided by OS (e.g. CPU utilization) and JVM  (e.g. garbage collection logs). Collected data are aggregated into a set of workload performance parameters that are closely watched from test to test. When a degradation (also called a regression) in performance of existing functionality or metric is observed, the performance team opens an investigation into the regression and drives it to full resolution.</p>
<h2>Memory Allocations Heavily Influence Application Performance</h2>
<p dir="ltr">Application performance depends on many factors, including: the architecture of the system,  algorithms used to achieve given functionality, efficiencies of the code and database queries, cache system, the database, and so on. Among these factors, object allocations play an important role in Java application performance, or any other application utilizing a VM that manages application memory. An increase in the number and/or size of objects allocated may take more operations by the application code.  Also, a higher object allocation rate usually leads to an increase in the overhead of memory management by the host VM.</p>
<p dir="ltr">Therefore, object allocations and JVM heap performance are one of the key metrics closely watched in the internal test workloads run by the Performance Engineering team. They are also closely monitored in production. A memory regression in a Java application usually results in an increase in the number of garbage collections (GCs) and their duration. Thus, basic GC statistics available through JVM logs (which are always turned on in our application using <span style="font-family: 'Courier New', Courier, monospace">-XX:+PrintGCDetails</span> flag of the JVM) can be used to monitor and detect Java memory regressions.</p>
<h2>Solving Memory Regressions in a Complex Application</h2>
<p dir="ltr">While detecting memory regressions is a relatively easy task, finding the root cause of the increase in memory allocations is usually a very hard problem to tackle. A number of commercial and open source tools exist that aim to help in solving this problem. Commercial tools like <a href="http://www.yourkit.com/">YourKit</a> can track object allocations by instrumenting bytecode of the application. Instrumentation is done by an agent attached to the JVM at startup. Another approach to solve memory regressions are heap dumps taken at runtime of the app and inspected later with tools like <a href="http://www.eclipse.org/mat/">Eclipse Memory Analyser (MAT)</a>, YourKit, etc. In addition to that, <a href="http://docs.oracle.com/javase/7/docs/jre/api/management/extension/com/sun/management/ThreadMXBean.html" target="_blank">ThreadMXBean</a> which is part of JMX MBeans can be used to estimate amount of memory allocated in a given transaction. This usually requires embedding an instrumentation framework in the application that collects and records this data in the logs for every transaction executed by the application.</p>
<p dir="ltr">Our experience shows that unfortunately, for a complex Java application, none of these methods guarantee solving memory allocation regressions. This is even more evident for minor regressions where the difference in memory allocation between compared code implementations is less pronounced, and here is why.</p>
<p dir="ltr">Java profilers that track memory allocations through instrumentation of bytecode, are usually not suitable for complex applications due to a very significant (10x-100x) overhead they add at runtime. The more complex the application is, and hence the more objects allocated during the run, the larger the overhead is. The overhead may be reduced by filtering out allocations of non-interesting classes. However, that requires significant research of the profiled code to identify classes that might be causing the memory regression. Even then, overhead may be significant. Also, due to complexity of the code, there is a chance that the classes whose objects caused the regression may be deemed as non-interesting and therefore be filtered out.</p>
<p dir="ltr">Another common approach, analyzing memory regressions with heap dumps taken at random moments of application runtime, rarely reveals the source of memory regression. This method may succeed when the regression is caused by a new class type introduced in the regressed version of the code. Therefore, comparing objects&#8217; class names found in the two heap dumps taken on different versions of the application might reveal the new class as the source of the memory regression. However, in a general case this approach of comparing heap dumps taken at random moments is rarely successful, especially when the difference in memory allocations is relatively small and the class names didn’t change. Content of a heap dump, even if taken at the same relative time during the workload run, highly depends on what and how many transactions were run, and how much time before the heap dump was taken a GC event happened. Hence, it is almost impossible to do apples-to-apples comparisons of two heap dumps taken during the workload run on different versions of the code.</p>
<p dir="ltr">Finally, the <span style="font-family: 'Courier New', Courier, monospace">ThreadMXBean</span> approach also has limitations because it only provides the amount of memory allocated by a given thread. It does not tell us what type of objects were allocated and in what part of the code. We use this approach, along with GC logs, as the first line of defense against memory regressions. Using <span style="font-family: 'Courier New', Courier, monospace">ThreadMXBean</span>, our code tracks the amount of memory allocated by every transaction and records it in the server log along with other performance parameters for the given transaction. Then, using log mining tools like <a href="http://www.splunk.com/">Splunk</a>, we analyze this data to pinpoint transactions that are the source of regression in a given workload.</p>
<h2>Collecting Information About Allocated Objects with Zero Overhead</h2>
<p dir="ltr">If these approaches do not help, what can we do to solve memory allocation regressions? Let’s summarize what we need to succeed:</p>
<ol>
<li>We want to record all object allocations and associated parameters (e.g. object type and amount of bytes allocated) during the run of our workload.</li>
<li>We do not want our workload to be impaired by overhead either caused by bytecode instrumentation, or overhead associated with collecting the data about allocated objects by the profiling agent.</li>
<li>We need to ensure high accuracy of the results collected in test experiments.</li>
</ol>
<p dir="ltr">At first sight, collecting all object allocations with no overhead might seem to be impossible to achieve, as any additional work requires some extra effort to accomplish the work. Unless…  memory allocations are already recorded for us for free! Yes, all objects are allocated on the heap. Hence, a heap dump of all objects (including unreachable ones) would contain all objects allocated by the application since the last garbage collection cycle.</p>
<p dir="ltr">However, as we noted earlier, simply taking a heap dump even at a predefined time instance does not help much in solving memory regression. Why? Because it may not contain all objects produced by a set of transactions we would like to analyze. It may not contain all objects because a GC event might have removed some of them and we are not in control of when JVM triggers a GC – unless we can implicitly control it!</p>
<p dir="ltr">How can we avoid a GC in a Java application? Relatively easy: run it on a host with an infinite amount of memory (RAM)! Nowadays, it is not uncommon for a developer to own a workstation with 64Gb of RAM. That amount of RAM can practically be considered an infinite amount of memory for a limited set of transactions we want to investigate. All we need to do is properly configure parameters of the JVM heap to avoid a GC during execution of these transactions.</p>
<p dir="ltr">As an example, consider throughput (parallel) JVM collector. For the purpose of this discussion, it is enough to know that <a href="http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html">heap of the JVM is split into Young generation and Old generation and the Young generation is further partitioned into Eden space and two Survivor spaces</a>. Size of the heap and its generations is specified by JVM parameters: <span style="font-family: 'Courier New', Courier, monospace">-Xmn</span>, the size of the Young generation, <span style="font-family: 'Courier New', Courier, monospace">-Xms</span>, the initial total size of the heap, and <span style="font-family: 'Courier New', Courier, monospace">-Xmx</span>, the maximum total size of the heap.  Size of Eden and Survivor spaces can be controlled by JVM flag <span style="font-family: 'Courier New', Courier, monospace">-XX:SurvivorRatio</span> that defines the ratio between Eden and each Survivor space. Thus, setting <span style="font-family: 'Courier New', Courier, monospace">-XX:SurvivorRatio</span> to a large number, forces JVM to dedicate most of the Young generation to Eden space.</p>
<p dir="ltr">For the throughput collector, there are 2 types of GC events: minor and full garbage collection. Minor collection is triggered when there is no available space in Eden to allocate new objects. A full GC is triggered when the space in the Old generation is not enough to accommodate objects promoted from the Young generation. As we noted earlier, our goal is to avoid any GC, as it removes allocated objects from the heap. Thus, for the throughput garbage collector, we need:</p>
<ol>
<li>Set the maximum and initial size of the heap for tested application to the maximum value not exceeding size of the RAM of the host where test is run.</li>
<li>Set the maximum size of Young generation very close to the maximum total size of heap.</li>
<li>Make Eden space occupy most of the Young generation by setting <span style="font-family: 'Courier New', Courier, monospace">-XX:SurvivorRatio</span> to a large number (e.g., 20).</li>
</ol>
<p dir="ltr">Note that even if the application we need to test uses different garbage collectors in production (that may also happen to have a different heap layout), nothing prevents us from changing the type of garbage collector to throughput collector and use the configuration described above. Changing the GC collector should not affect object allocations in the application.</p>
<p dir="ltr">Once we configure heap for the size that allows the largest amount of object allocations without garbage collection, we need to focus on the workload where we observed the memory allocation regression:</p>
<ol>
<li>Identify workload transactions that contribute the largest amount to the memory regression.</li>
<li>Limit memory allocations performed during the workload run to the amount available in the young generation of the heap we configured earlier.</li>
</ol>
<p dir="ltr">To identify transactions that contribute the most to the memory regression, we use information from the application logs collected with the help of <span style="font-family: 'Courier New', Courier, monospace">ThreadMXBean</span>. If this information is not available in your application, transactions that contribute the most to the regression can be identified by re-running the workload with only a single type of transaction for each type involved in the workload, and then using GC logs to identify which run (and thus type of transaction) shows the largest regression in memory allocations.</p>
<p dir="ltr">Having identified type of transaction that is the largest contributor to memory regression, we need to modify the workload to run this type of transaction only. We will then run it for a duration and frequency that would allow fitting all objects allocated during the run in the Eden space we configured.</p>
<h2>Runtime Phases of a Typical Workload</h2>
<p dir="ltr">A typical workload has a startup (warm-up) phase followed by a steady state phase. In the startup phase the application is initialized, the cache is warmed up, and so on. In the steady state phase the load to the system under test does not vary much and the transaction response time has a relatively low variance. We are interested in investigating the steady state part of the workload, not the transactions and memory allocations happening in the startup phase.</p>
<p><img class="alignnone size-full wp-image-32211" alt="app-phases-warmup-steadystate_jkeodq" src="https://res.cloudinary.com/hzxejch6p/image/upload/v1416270072/app-phases-warmup-steadystate_jkeodq.png" width="850" height="500" /></p>
<address>Figure 1. Heap occupancy and different phases of a running Java app</address>
<p dir="ltr">Phases of running Java applications can be identified by monitoring the JVM heap, which can be done with the help of various tools (e.g., <a href="https://docs.oracle.com/javase/7/docs/technotes/guides/management/jconsole.html">JConsole</a>). JConsole can be attached to a running JVM process and allows us to see how heap occupancy changes over time as the workload runs. The startup phase and steady state phase generally differ by the memory allocation rate (see Figure 1). Looking at the chart of heap occupancy during the steady state we can determine how long our workload can run between consecutive GC events. If that duration is too short, the load and hence memory allocations can be reduced by tuning parameters of the workload (e.g., number of concurrent threads in JMeter that implement the transaction we choose to run). To increase accuracy of the test results, we recommend running a fixed number of transactions during the workload, as opposed to a fixed duration workload.</p>
<p dir="ltr">We now know what transactions we should run and how many of them we can run before a GC happens, which takes us to the algorithm for recording all memory allocations without any overhead:</p>
<ol>
<li>Start the profiled application with heap parameters enabling the largest size of Eden space for the given hardware (RAM).</li>
<li>Attach a heap monitoring tool (JConsole) to the JVM process.</li>
<li>Monitor heap occupancy and identify when application startup phase is over.</li>
<li>If a warm-up of the application is required, run the workload with the set of transactions causing the largest memory regression for a sufficient amount of time.</li>
<li>After the warm-up period is over, trigger full GC for the JVM process to clean the heap from objects allocated during startup/warm-up phases.</li>
<li>Run the workload while monitoring heap occupancy to make sure no GC happens during the run of the workload.</li>
<li>Record objects allocated during the steady state of the workload by triggering heap dump for the JVM process (<span style="font-family: 'Courier New', Courier, monospace">jmap -dump:format=b,file=hd.hprof &lt;pid&gt;</span>) that will include all objects (live and unreachable).</li>
</ol>
<p>This algorithm needs to be repeated for the baseline and regressed versions of the code. The two heap dumps produced as the result of the algorithm will contain all objects allocated during the steady state phase of the run. Thus, comparison of the content of the heap dumps with tools like Eclipse MAT or YourKit should reveal differences in the type/number/size of objects allocated.</p>
<h2>Example</h2>
<p dir="ltr">Consider an example of profiling a simple Java application that allocates <span style="font-family: 'Courier New', Courier, monospace">double</span> arrays wrapped in class <span style="font-family: 'Courier New', Courier, monospace">MyDoubleArray</span> (see Appendix for the code of this example). In the base run, the application allocates 2000 <span style="font-family: 'Courier New', Courier, monospace">MyDoubleArray</span> objects each containing a <span style="font-family: 'Courier New', Courier, monospace">double[102400]</span> array. The regressed version of the code allocates the same number of <span style="font-family: 'Courier New', Courier, monospace">MyDoubleArray</span> objects, but with the double array larger by 100 elements (i.e., <span style="font-family: 'Courier New', Courier, monospace">double[102500]</span>). Comparison of heap dumps produced as the result of the algorithm we discussed show this difference in the amount of memory occupied by <span style="font-family: 'Courier New', Courier, monospace">double[]</span> (see Figure 2). Note that other objects have the same count and occupy the same space in the 2 heap dumps.</p>
<p dir="ltr"><img alt="base_objectallocation.png" src="https://res.cloudinary.com/hzxejch6p/image/upload/v1416270558/base_objectallocation_rle15g.png" width="420px;" height="325px;" /> <img alt="regressed_objectallocations.png" src="https://res.cloudinary.com/hzxejch6p/image/upload/v1416270558/regressed_objectallocations_kppqxx.png" width="420px;" height="325px;" /></p>
<address>Figure 2. Object allocations recorded in the heap dump of base run (left) and regressed run (right).</address>
<h2>Key Takeaways</h2>
<ul>
<li>Object allocations in a Java application heavily influence its performance, hence avoiding regressions in this area is very important.</li>
<li>Existing tools and methods even when systematically combined, may not help in solving Java memory regressions; this is even more true for complex applications and smaller regressions.</li>
<li>The presented algorithm can record object allocations with zero overhead and high accuracy that can be used to identify root cause of any memory regression in a complex Java application.</li>
</ul>
<h2>Appendix</h2>
<p><strong>Java heap parameters provided at startup of the ObjectAllocator application:</strong></p>
<p><span style="font-family: 'Courier New', Courier, monospace">-XX:+UseParallelOldGC -Xmn3900m -Xms4396m -Xmx4396m -XX:SurvivorRatio=20 -XX:MaxPermSize=100m</span></p>
<p><strong>Code of ObjectAllocator application used in the example</strong></p>
<pre>import java.io.*;

public class MyDoubleArray {
	private double[] arr;

	MyDoubleArray(int size) {
		arr=new double[size];
	}
}

public class ObjectAllocator {
	private static void runAllocationsSingleThread(long iterations, int objectSize, 
                                                       boolean regressed) {
		int regressedObjectSize=100;
		int objects=10;
		MyDoubleArray tmpObj;
	    	long mainSleepTime=(long)(0.1*1000);

		if (regressed) 
			objectSize+=regressedObjectSize;
		for (int i=0; i&lt;iterations;i++) {
			for(int j=0;j&lt;objects;j++){
				tmpObj=new MyDoubleArray(objectSize);
			}
		}
	}

	public static void main(String args[]) throws IOException {
		BufferedReader in = new BufferedReader(new InputStreamReader(System.in));
		String s;
		System.out.println("Press enter to start warmup");
		s=in.readLine();
		runAllocationsSingleThread(100,1500*1024,false);
		System.out.println("Run increased memory allocations (true/false)?");
		boolean regressed=Boolean.parseBoolean(in.readLine());
		if (regressed)
			System.out.println("Running regressed application");
		else
			System.out.println("Running baseline application");
		runAllocationsSingleThread(200,100*1024,regressed);
                System.out.println("main run completed, press enter");
		s=in.readLine();
	   }
}</pre>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/SforceBlog?a=p-IABIgItXs:AOIpZuua5oA:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=p-IABIgItXs:AOIpZuua5oA:qj6IDK7rITs"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=qj6IDK7rITs" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=p-IABIgItXs:AOIpZuua5oA:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=p-IABIgItXs:AOIpZuua5oA:V_sGLiPBpWU" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=p-IABIgItXs:AOIpZuua5oA:F7zBnMyn0Lo"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=p-IABIgItXs:AOIpZuua5oA:F7zBnMyn0Lo" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=p-IABIgItXs:AOIpZuua5oA:l6gmwiTKsz0"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=l6gmwiTKsz0" border="0"></img></a>
</div><img src="//feeds.feedburner.com/~r/SforceBlog/~4/p-IABIgItXs" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://developer.salesforce.com/blogs/engineering/2014/12/solving-java-memory-regressions-high-accuracy-zero-overhead.html/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		<feedburner:origLink>http://developer.salesforce.com/blogs/engineering/2014/12/solving-java-memory-regressions-high-accuracy-zero-overhead.html</feedburner:origLink></item>
		<item>
		<title>Master Visualforce: Top Must Watch Videos!</title>
		<link>http://feedproxy.google.com/~r/SforceBlog/~3/4poNMQWc8Ac/visualforce-top-videos-webinars.html</link>
		<comments>http://developer.salesforce.com/blogs/developer-relations/2014/12/visualforce-top-videos-webinars.html#comments</comments>
		<pubDate>Tue, 02 Dec 2014 16:00:42 +0000</pubDate>
		<dc:creator><![CDATA[Sonia Advani]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[visualforce]]></category>
		<category><![CDATA[Visualforce Page]]></category>

		<guid isPermaLink="false">http://26.91341</guid>
		<description><![CDATA[<a href="http://developer.salesforce.com/blogs/developer-relations/2014/12/visualforce-top-videos-webinars.html"><img align="right" hspace="5" width="150" src="https://res.cloudinary.com/hzxejch6p/image/upload/v1417547978/2014-300x90-vf-blog_stw4bg.png" class="alignright wp-post-image tfe" alt="Visualforce Webinars and Videos" title="Visualforce Webinars and Videos" /></a>Learn how to quickly build cloud apps with beautiful UIs with these hand selected webinars and videos.]]></description>
				<content:encoded><![CDATA[<p><img class="alignleft wp-image-91751" title="Visualforce Webinars and Videos" alt="Visualforce Webinars and Videos" src="https://res.cloudinary.com/hzxejch6p/image/upload/v1417547978/2014-300x90-vf-blog_stw4bg.png" width="300" height="90" /></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>Visualforce is Force.com’s component-based framework that allows you to quickly build pages and add UI elements using a custom markup language. Because it is a markup language, you can also embed HTML5 and JavaScript including popular JS frameworks and libraries, making it easy to build visually striking applications that can create, read, update, delete and query Salesforce objects.</p>
<p>Building UIs with Visualforce can be a lot of fun, and learning the tricks just as enjoyable. So, here are some of our most popular and very informational Visualforce video recordings presented by our expert developers. Watching these recordings will help take your Visualforce knowledge to the next level.</p>
<p>&nbsp;</p>
<h2>Top Visualforce Video Recordings to Watch</h2>
<p><strong>1)</strong> New to Visualforce, or looking for an introductory refresher? Our <a title="Intro to Visualforce" href="http://youtu.be/jRwCUW4VYJc">Intro to Visualforce.com</a> video is a 30-minute introduction to this topic, followed by a great and question and answer session at the end.</p>
<p><iframe width="500" height="375" src="//www.youtube.com/embed/jRwCUW4VYJc?feature=oembed" frameborder="0" allowfullscreen></iframe></p>
<p><strong>2)</strong> Looking for a more advance overview on Visualforce? Check out our video on <a title="Webinar - Advanced Visualforce" href="https://www.youtube.com/watch?v=IbS6rmPZkbM&amp;amp;list=PL15CC382AFE752131&amp;amp;index=58">Advanced Visualforce</a>. It takes you through an in-depth tour of  Visualforce, to help you improve page efficiency, and provide you with thorough knowledge of Viewstate, JavaScript Remoting, Asynchronous Apex and the Streaming API.</p>
<p><iframe width="500" height="281" src="//www.youtube.com/embed/IbS6rmPZkbM?start=2&#038;feature=oembed" frameborder="0" allowfullscreen></iframe></p>
<p><strong>3)</strong> Seeking even more in-depth information on Visualforce? Check out our video recordings on <a title="Visualforce &amp;amp; Force.com Canvas: Unlock your Web App inside of Salesforce.com" href="http://youtu.be/AchJBotmI2s">Visualforce &amp; Force.com Canvas: Unlock your Web App inside of Salesforce.com</a>.</p>
<p><iframe width="500" height="281" src="//www.youtube.com/embed/AchJBotmI2s?feature=oembed&#038;start=7" frameborder="0" allowfullscreen></iframe></p>
<p><strong>4)</strong> Finally, check out <a title="Visualforce in Salesforce1: Optimizing your User Interface for Mobile" href="https://www.youtube.com/watch?v=R3ytoc2MB60&amp;amp;list=PL15CC382AFE752131&amp;amp;index=23">Visualforce in Salesforce1: Optimizing your User Interface for Mobile</a> where you will explore how you can customize the Salesforce1 Mobile app using Visualforce, as well as learning best practices for developing mobile-optimized Visualforce pages for Salesforce1.</p>
<p><iframe width="500" height="281" src="//www.youtube.com/embed/R3ytoc2MB60?feature=oembed" frameborder="0" allowfullscreen></iframe></p>
<p><span style="line-height: 1.5em;">In addition to these videos, be sure to visit the </span><a style="line-height: 1.5em;" title="Visualforce Resource Page" href="https://developer.salesforce.com/page/Visualforce">Visualforce Resource Page</a>,<span style="line-height: 1.5em;"> as well as checking out our </span><a style="line-height: 1.5em;" title="Force.com Developer Events" href="https://developer.salesforce.com/calendar">Developer Events page</a>. W<span style="line-height: 1.5em;">e have some new Visualforce related webinars planned in the upcoming months, so please be on the look out!</span></p>
<p>What can you do with Visualforce? We hope these helpful videos will spark your imagination. If you have any Visualforce-related questions, please post your questions on the <a title="Visualforce Discussion forum" href="https://developer.salesforce.com/forums/#!/feedtype=RECENT&amp;amp;dc=Visualforce_Development&amp;amp;criteria=ALLQUESTIONS">Visualforce Discussion Forum</a> or <a title="Force.com on Twitter" href="https://twitter.com/SoniaAdvani">find me on Twitter</a>.</p>
<p><img class="alignleft wp-image-91781" alt="" src="https://res.cloudinary.com/hzxejch6p/image/upload/v1417548333/Sonia_150px_aw4oby.jpg" width="100" height="150" /></p>
<p>Best Wishes,<br />
Sonia Advani</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/SforceBlog?a=4poNMQWc8Ac:I-iZa20d80s:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=4poNMQWc8Ac:I-iZa20d80s:qj6IDK7rITs"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=qj6IDK7rITs" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=4poNMQWc8Ac:I-iZa20d80s:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=4poNMQWc8Ac:I-iZa20d80s:V_sGLiPBpWU" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=4poNMQWc8Ac:I-iZa20d80s:F7zBnMyn0Lo"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=4poNMQWc8Ac:I-iZa20d80s:F7zBnMyn0Lo" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=4poNMQWc8Ac:I-iZa20d80s:l6gmwiTKsz0"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=l6gmwiTKsz0" border="0"></img></a>
</div><img src="//feeds.feedburner.com/~r/SforceBlog/~4/4poNMQWc8Ac" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://developer.salesforce.com/blogs/developer-relations/2014/12/visualforce-top-videos-webinars.html/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		<feedburner:origLink>http://developer.salesforce.com/blogs/developer-relations/2014/12/visualforce-top-videos-webinars.html</feedburner:origLink></item>
		<item>
		<title>Visual Development – When to Click Instead of Write Code</title>
		<link>http://feedproxy.google.com/~r/SforceBlog/~3/FAL4LFE8vME/forcedotcom-declarative-development.html</link>
		<comments>http://developer.salesforce.com/blogs/engineering/2014/12/forcedotcom-declarative-development.html#comments</comments>
		<pubDate>Mon, 01 Dec 2014 12:00:05 +0000</pubDate>
		<dc:creator><![CDATA[markus-spohn]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[architect]]></category>
		<category><![CDATA[declarative]]></category>
		<category><![CDATA[development]]></category>

		<guid isPermaLink="false">http://25.33231</guid>
		<description><![CDATA[<a href="http://developer.salesforce.com/blogs/engineering/2014/12/forcedotcom-declarative-development.html"><img align="right" hspace="5" width="150" src="https://res.cloudinary.com/hzxejch6p/image/upload/v1417031657/a_aaa-A-triangular-bicycle-wheel_zicbmx.jpg" class="alignright wp-post-image tfe" alt="" title="Force.com Declarative Development" /></a>As Salesforce developers,  we love to write code! But does it follow that because we can we should? Check out these declarative development options for building great apps without writing a single line of code.]]></description>
				<content:encoded><![CDATA[<p><img class="alignright wp-image-33341" title="Force.com Declarative Development" alt="" src="https://res.cloudinary.com/hzxejch6p/image/upload/v1417031657/a_aaa-A-triangular-bicycle-wheel_zicbmx.jpg" width="288" height="175" />Let’s face it, as Salesforce developers we aren’t really all that different from other developers &#8211; we love to write code! And just because we can, we sometimes write code for things that could have easily been solved without coding, or we use our skills to address even the last tiny functionality or usability gap in a standard page… and end up writing a complete new custom user interface and maybe even framework along the way.</p>
<p>While custom development sure is a lot of fun &#8211; after all we’re developers and that’s what we do &#8211; the question is whether it’s really such a great idea to always dive straight into code when customizing Salesforce or building a new app on the Salesforce platform. Force.com started out as a declarative development platform in its early days and continues to provide a rich set of declarative development features to this day, so let’s dig in and find out what we can (and should) do with clicks instead of writing code.</p>
<h1>Write Code for Everything?</h1>
<p>At first sight it might sound like a really good idea. If everything is implemented in code, everything is in one place, easy to find and it follows the same basic approach and methodology. We get to build trigger frameworks, helper classes, service layers, etc. and we get to build it exactly how we want it. Sounds great, doesn’t it?</p>
<p>But let’s consider a few things on the flip side:</p>
<ul>
<li><strong>Cost and time</strong> &#8211; what will the resources for all this custom development cost and how long will it take?</li>
<li><strong>Maintenance</strong> &#8211; who’s going to maintain the code after the initial development is done?</li>
<li><strong>Complexity and Scalability</strong> &#8211; code can introduce various degrees of complexity. More code typically introduces more complexity and complexity can be hard to manage in terms of scalability. E.g. if your implementation heavily depends on business logic coded with Apex code, you are more likely to run into governor limits.</li>
</ul>
<p>As you can see, there are quite a few things to consider here. But let’s flip the argument around again and see why we maybe wouldn’t want to write a single line of code at all.</p>
<h2>Clicks not Code!</h2>
<p>And by clicks I really mean Salesforce native functionality or declarative development features that allow us to build new functionality or customize without writing any code. With this approach we are a bit more limited in terms of building exactly what we want, but the reasons why this approach is so powerful are quite simple &#8211; let’s first use the same dimensions as above again:</p>
<ul>
<li><strong>Cost and time</strong> &#8211; using declarative development features is fast! It doesn’t involve writing code, writing test classes, worrying about checking in code into a repository, version control your code, etc.</li>
<li><strong>Maintenance</strong> &#8211; probably one of the biggest differences. There is no code and test automation needs to be maintained when using native features and declarative customizations</li>
<li><strong>Complexity and Scalability</strong> &#8211; we can still build a lot of complex things without writing a single line of code, but there is less to worry about in terms of governor limits as they don’t apply to declarative customization. There are of course limits to keep in mind for declarative customizations as well, but the main difference is that these are design limits (e.g. total number of workflow rules on an object) rather than execution governor limits (e.g. total number of SOQL queries issued), so they’re easier to handle with a thoughtful design approach.</li>
</ul>
<p>And I will add two more things to the list above:</p>
<ol>
<li>Update Path &#8211; Native features and declarative customizations will automatically update hand-in-hand with each Salesforce release. Whenever Salesforce improves a native or declarative features, we get to take advantage of the improvement without really having to do anything.<br />
While code customizations will never break after a Salesforce release, there’s might be some level of effort required to work newly released Apex features into our code, or &#8211; if we used code to customize a native feature &#8211; to adapt our code to fit the new and improved native feature.</li>
<li>Usability and User Productivity &#8211; Sometimes using standard features and declarative customizations can keep the usability of the application more consistent. One example is using a standard Edit page vs. overriding it with a building a custom Visualforce Edit page. On the Pro side for building a Visualforce page is that we can build the Edit page exactly the way we want it. But there are also trade offs for overriding a standard Edit page that aren’t always considered and that can have a big impact on the consistent usability of the app and user productivity:
<ul>
<li>We lose the ability to easily change the page layout using the Page Layout Editor</li>
<li>We lose inline editing capabilities in list views</li>
</ul>
</li>
</ol>
<p>So let’s find out what we can all do with clicks!</p>
<h1>Examples for Declarative Development versus Code</h1>
<p>Here are some use cases and examples for functionality that can easily be build declaratively, without writing a single line of code:</p>
<ul>
<li>Instead of writing <strong>Triggers</strong>, we can automate Field Updates using <strong>Workflow</strong> &#8211; automatically populating a field with a default value or updating a field based on the value of another field is a pretty common requirement. Workflow can address the basic use cases just as well as writing an Apex Trigger.</li>
<li>Use <strong>Formula Fields</strong> and <strong>Roll-Up Summary Fields</strong> for field calculations instead of writing a <strong>Visualforce</strong> page and calculate the field values in a controller extension &#8211; a good example is a simple Order Management app. On the Order Lines, the order line total is calculated by multiplying the item price with the ordered quantity. A formula field can easily achieve this. And if we want to have the sum of all order line totals on the order header, we can use a Roll-up Summary field, as long as there is a Master-Detail relationship between the order and the order lines object.</li>
<li>Enforce Business Rules with <strong>Validation Rules</strong> whenever possible instead of <strong>Triggers</strong> and <strong>code</strong><br />
Don’t want to allow users to save the order if a piece of information is missing? Validation Rules is a fast and easy-to-use alternative to writing custom Visualforce pages and controllers or Apex Triggers.</li>
<li>Use <strong>Approval Processes</strong> and <strong>Flows</strong> to implement logic and processes<br />
A lot of complex custom business logic and business processes can be defined using these two powerful tools. And probably the nicest benefit is that Approval Processes and Flows visualize the process, which makes it much easier to understand what’s going on than looking at lines and lines of Apex code.</li>
<li>Using <strong>Standard vs. Custom Objects</strong><br />
We see this more often than one would think. Before creating a new custom object, check if there’s a standard object available that can address the functional requirements and/or can be customized to close the gap. Not only will this potentially save a lot of time, but also help control the number of custom objects in Salesforce, which is limited as we all know. A great example here is the Orders object that was released with Salesforce Spring ‘14. Before the Orders object was a Standard object, developers spent a lot of time and resources building functionality that is now available out-of-the-box. Keep in mind that there are three major Salesforce releases per year, so Salesforce is constantly adding new features and functionality. It’s always a good idea to check both the <a title="Salesforce1 Platform Release Notes" href="http://www.salesforce.com/customer-resources/releases/winter15/">Salesforce1 Platform Release Notes</a> and the <a title="Force.com Platform Release Notes and Summary" href="https://developer.salesforce.com/releases/release/Winter15">Force.com Release Pages</a> here on Salesforce Developers  for new standard objects and/or new features that may have required custom building in the past.</li>
</ul>
<h2>Summary</h2>
<p>The main point I’m conveying is that as Salesforce developers we should not always dive straight into coding just because we can. Salesforce offers a rich set of declarative development features and before implementing customizations or new functionality, we should always make sure that we are familiar and keep up to date with Salesforce’s native capabilities and that we have exhausted all native options before we start to code and reinvent the wheel.</p>
<p>Using native functionality or declarative development features has many benefits from time and cost savings to easier maintenance and sometimes better usability.</p>
<p>Of course not everything can be built without coding, and there is a time and plenty of places when writing code is the right approach to build something. Make yourself aware of the native and declarative-development features in Salesforce before you code, and evaluate them in your design before you consider coding.</p>
<h1>Additional Resources</h1>
<p>Here are some more useful sites and blog posts that talk about Salesforce development without writing code:</p>
<p><a href="https://developer.salesforce.com/page/An_Introduction_to_Point-and-Click_App_Building_with_Force.com">An Introduction to Point-and-Click App Building with Force.com</a><br />
<a href="http://blogs.salesforce.com/product/2012/07/you-can-build-apps-no-code-required.html">You Can Build Apps: No Code Required</a><br />
<a href="http://www.salesforce.com/us/developer/docs/workbook/index_Left.htm#CSHID=workshop_120_intro.htm%7CStartTopic=Content%2Fworkshop_120_intro.htm%7CSkinName=webhelp">Force.com Workbook &#8211; Chapter Add App Logic with Clicks, Not Code</a><br />
<a href="http://www.salesforce.com/us/developer/docs/fundamentals/salesforce_creating_on_demand_apps.pdf">Force.com Platform Fundamentals &#8211; An Introduction to Custom Application Development in the Cloud</a><br />
<a href="https://developer.salesforce.com/page/App_Logic">Point-and-Click App Logic</a></p>
<h1>About the Author and CCE Technical Enablement</h1>
<p><a href="https://twitter.com/markus_spohn">Markus Spohn</a> is a Technical Enablement Architect within the Technical Enablement team of the Salesforce Customer-Centric Engineering group. The team’s mission is to help customers understand how to implement technically sound Salesforce solutions. Check out all of the resources that this team maintains on the <a href="https://developer.salesforce.com/architect">Architect Core Resources</a> page of Developer Force.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/SforceBlog?a=FAL4LFE8vME:YadYkYLZRec:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=FAL4LFE8vME:YadYkYLZRec:qj6IDK7rITs"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=qj6IDK7rITs" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=FAL4LFE8vME:YadYkYLZRec:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=FAL4LFE8vME:YadYkYLZRec:V_sGLiPBpWU" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=FAL4LFE8vME:YadYkYLZRec:F7zBnMyn0Lo"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=FAL4LFE8vME:YadYkYLZRec:F7zBnMyn0Lo" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=FAL4LFE8vME:YadYkYLZRec:l6gmwiTKsz0"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=l6gmwiTKsz0" border="0"></img></a>
</div><img src="//feeds.feedburner.com/~r/SforceBlog/~4/FAL4LFE8vME" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://developer.salesforce.com/blogs/engineering/2014/12/forcedotcom-declarative-development.html/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		<feedburner:origLink>http://developer.salesforce.com/blogs/engineering/2014/12/forcedotcom-declarative-development.html</feedburner:origLink></item>
		<item>
		<title>Integration Architecture for the Salesforce Platform</title>
		<link>http://feedproxy.google.com/~r/SforceBlog/~3/Dgz5dBUWv6Y/salesforce-integration-architecture.html</link>
		<comments>http://developer.salesforce.com/blogs/developer-relations/2014/11/salesforce-integration-architecture.html#comments</comments>
		<pubDate>Wed, 26 Nov 2014 19:42:05 +0000</pubDate>
		<dc:creator><![CDATA[Greg Cook]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[architecture]]></category>
		<category><![CDATA[Enterprise Architecture]]></category>
		<category><![CDATA[Integration Architecture]]></category>

		<guid isPermaLink="false">http://26.88041</guid>
		<description><![CDATA[<a href="http://developer.salesforce.com/blogs/developer-relations/2014/11/salesforce-integration-architecture.html"><img align="right" hspace="5" width="150" src="https://res.cloudinary.com/hzxejch6p/image/upload/v1418669920/Greg_Cookx150_m1ke1a.jpg" class="alignright wp-post-image tfe" alt="" title="Force.com Enterprise Architecture" /></a>Editors Note: This post is part of a &#8220;Guest&#8221; series entitled Enterprise Architecture with Force.com. Our guest blogger, Greg Cook, is a managing partner of CloudPremise and currently holds all seven Salesforce certifications. What are the components of a good Salesforce Integration Architecture? As a Salesforce Architect it is your role to lead your company in the evolution [&#8230;]]]></description>
				<content:encoded><![CDATA[<p><em>Editors Note: This post is part of a &#8220;Guest&#8221; series entitled Enterprise Architecture with Force.com. Our <em>guest blogger, </em>Greg Cook, is a managing partner of CloudPremise and currently holds all seven Salesforce certifications.</em></p>
<p><img class="alignleft wp-image-92511" title="Force.com Enterprise Architecture" alt="" src="https://res.cloudinary.com/hzxejch6p/image/upload/v1418669920/Greg_Cookx150_m1ke1a.jpg" width="122" height="150" />What are the components of a good Salesforce Integration Architecture? As a Salesforce Architect it is your role to lead your company in the evolution of it’s Integration Architecture. A good architect must understand both integration architecture and integration patterns. The difference between the two is analogous to designing the highway versus driving cars on the highway. The Salesforce1 Platform offers architects and developers a wide array of integration technologies and recommended patterns (the cars). However, without the correct Integration Architecture and technology infrastructure (the highway) your projects and solutions will be at risk for performance, scalability, data integrity, and many other problems. This post shares patterns gathered from my experience in creating an effective Integration Architecture for clients, and shares a reference design drawn from many of these. You should also consult the official <a href="http://www.salesforce.com/us/developer/docs/integration_patterns/integration_patterns_and_practices.pdf">Salesforce Integration Patterns guide</a> when architecting your Salesforce solutions.</p>
<h2>The Integration Architecture Aligns the Business Strategy with Technical Capabilities</h2>
<p>The best Salesforce Architectures are not based upon incumbent technology, singular architecture approaches, or corporate politics. The best Salesforce Architectures are based upon delivering business value. What this means for the architect is to focus on what are the business’s requirements, roadmap, and needs for which you will offer technical capabilities. In other words – you need to see where the business wants to drive, and figure out which highways and roads are necessary to support the amount of traffic. Idealistic architecture (for example, 100% Services Oriented Architecture) may cripple your ability to provide the capabilities needed by your business when they need them.</p>
<h2>The Integration Architecture supports a Mix of Batch Processing and Real-time Services Middleware</h2>
<p>Good Salesforce architects have learned that the best integration designs support both batch and service-based patterns.  This means having multiple types of middleware at work.  I have had clients with three to four different integration platforms in their Salesforce architectural landscape.  This is because no single solution can effectively meet all of your requirements, and once again the idealistic architectures are not as important as supporting the business&#8217;s needs.</p>
<h2>The Integration Architecture is Based Upon Business Service Level Agreements (SLAs)</h2>
<p>A mature organization and architect will attempt to define SLAs for data and process integrations. These SLAs have an important role on projects as they may radically affect the chosen technology and integration pattern. The SLAs should be based upon real business needs (sorry – not everything in life needs to be real time) that help define the non-functional requirements. If you only need to drive a few miles you do not need a highway. However if you are going on a road-trip I hope you aren’t taking side-roads! Define your solutions based upon your business’s service-level requirements.</p>
<p>One important aspect of managing SLA’s on the Salesforce1 platform is to understand if and when to use asynchronous processing. The multi-tenant design of Salesforce makes it impractical to use asynch patterns when conforming to SLAs. A batch or @future job may not finish processing as quickly as needed, and there is very little you can do about that (apart from adjusting the SLAs). But as CCE Principal Architect Bud Vieira points out, &#8220;This may depend on whether your SLAs are determined simply by response times of individual queries, inserts, deletes and updates (as in a transactional process), or throughput of an overall operation. For example, time to complete a stock buy versus how quickly a sales account realignment process can be completed.&#8221;</p>
<h2>The Integration Architecture Has a Clearly Defined Standard for Applying Different Integration Use Cases</h2>
<p dir="ltr">As your landscape evolves and your Salesforce expertise matures, the goal is to define a set of capabilities and standards for all Salesforce integrations at your company.  Each project should not have to define when and where to use what technologies, how and when to authenticate, etc.  These architecturally significant designs should be standardized for your enterprise.  This is where a Center of Excellence or Architecture Review Board comes into play.  Each Salesforce project should be subservient to a higher level integration architecture authority that is typically governed outside of the Salesforce1 platform teams.</p>
<h2>A Typical Enteprise Salesforce Integration Architecture</h2>
<p>Let&#8217;s take a look at a reference Salesforce Integration Architecture.  This may or may not look like your existing landscape &#8211; however this reference is based upon years of work at many Fortune 500 companies.  The reference design also does not recommend one technology vendor or solution over another &#8211; rather the goal is to understand the technical capabilities that you can (and probably should) consider as your Salesforce Platform landscape matures.</p>
<p><img class="aligncenter wp-image-91211" title="Force.com Enterprise Architecture" alt="Force.com Enterprise Architecture Greg Cook" src="https://res.cloudinary.com/hzxejch6p/image/upload/c_scale,w_517/v1417024231/reference-integration-architecture_frhefm.png" width="517" height="388" /></p>
<p>Let’s take a look at the most common integration use-cases and how they apply to your Salesforce Integration Architecture.  The direction of the arrows in the reference model is not necessarily the way the data is moving, but rather the way the integration connection is being established.  This is a critical aspect of Integration Architecture as it pertains to your security and any real-time requirements.</p>
<h2>Cloud-to-Ground (Salesforce Platform Originated)</h2>
<p dir="ltr">In Cloud-to-Ground use cases you are attempting to push a transaction (message or data) from Salesforce into your On-Premise infrastructure.</p>
<p dir="ltr"><strong>Capability #1</strong> &#8211; The Salesforce originated message is relayed to a DMZ (demilitarized zone) service end-point.  This could be a firewall, a services gateway appliance, or reverse proxy.  You must work closely with your security team to define this layer as opening the corporate firewall to inbound web traffic is a high security risk.  This is where much (if not all) of your security authentication from Salesforce Platform occurs.  Whitelisted IPs, two-way SSL, and basic HTTP authentication are some of the ways to authenticate Salesforce into the DMZ layer.</p>
<p dir="ltr"><strong>Capability #2 </strong>- The message is relayed from the DMZ security zone into the trusted On-Premise infrastructure.  The message is usually destined for an Enterprise Service Bus (ESB) and durable message queue.  The ESB also would handle any transformation, mediation, and orchestration services required by the detailed integration requirements.</p>
<p dir="ltr"><strong>Capability #3</strong> &#8211; Depending on your Enterprise Architecture the ESB maybe pushing the message into the SOA infrastructure.  These web-services are providing consumer agnostic data and business process services to the Enterprise.  The Salesforce Platform can become a consumer (and later a producer) of these SOA services.  By reusing existing SOA web-services you can save your project a lot of time and money as opposed to integrating directly into the source system.  If you do not have a SOA layer your project may be responsible for integrating directly into the legacy application.</p>
<p><strong>Capability #4</strong> &#8211; Another key capability for mature Salesforce Integration Architectures is for some sort of On-Premise database access.  This maybe a standalone database or part of a more formal Enterprise Data Warehouse (including an ODS – an operational data store).  Most commonly (but not always) in a Cloud-to-Ground scenario this transaction would be a database READ.  The Salesforce Platform can read data from the database in real (or near-real) time.</p>
<h2>Ground-to-Cloud (On-Premise Originated)</h2>
<p dir="ltr">In Ground-to-Cloud use cases you are attempting to push AND pull data from Salesforce from your On-Premise infrastructure.</p>
<p dir="ltr"><strong>Capability #5</strong> &#8211; A mature Integration Architecture should be handling all of the real-time calls into Salesforce from the ESB.  However if you do NOT have an ESB, this step would occur from each separate application requiring access to Salesforce.  From a security standpoint it is much better to handle all of the calls to Salesforce from a centralized integration middleware.  You can use oAuth or user/pw session based authentication to Salesforce.  The middleware may already have a session with Salesforce so that you would not need to authenticate again for every transaction.</p>
<p dir="ltr"><strong>Capability #6</strong> &#8211; Many integrations can be accomplished in a batch design.  This is often the cheapest and fastest way to get data in and out of the Salesforce Platform.  I would recommend a robust ETL solution for all Salesforce environments.  The role of the ETL is to move large data volumes using the Bulk API where possible.</p>
<p dir="ltr"><strong>Capability #7</strong> &#8211; As a Salesforce architect you have a responsibility to your company or client to off-load your Salesforce data into a replicated copy.  My argument for this is that while Salesforce’s database is not likely to have outages or lose data, you and your team are VERY likely to break your own data via user error, bad-code, or run-away processes.  By replicating your data off-line you now have the power to restore data to an earlier state without engaging Salesforce (who may or may not be able to restore it exactly as necessary). A key architecture choice here, although it may seem trivial, is to make sure these kinds of replications are processing diffs, and never whole tables whether records have changed or not. It&#8217;s a scalability killer, and so needs to be thought out up front.</p>
<p><strong>Capability #8</strong> &#8211; The ETL is also responsible for moving data in and out of your database infrastructure.  Often data is necessary to be staged into Salesforce (Accounts for example) from the EDW.  Also pulling data down from Salesforce into your EDW may be much easier when done using batch processing patterns.</p>
<h2>Cloud-to-Cloud</h2>
<p dir="ltr"><strong>Capability #9</strong> &#8211; If you have multiple orgs (<a title="Force.com Enterprise Architecture: Single-org versus Multi-org Strategy" href="https://developer.salesforce.com/blogs/developer-relations/2014/10/enterprise-architecture-multi-org-strategy.html">Single-org versus Multi-org Strategy</a>) you will often have the need to integrate between the Orgs.  Salesforce makes this (sometimes too) easy via Salesforce2Salesforce.  You can also directly contact another Org via RESTful web services integration.  The Salesforce Platform’s road-map includes the ability to consume other org’s data via <a href="https://help.salesforce.com/apex/HTViewHelpDoc?id=cod_overview.htm">Hub/Spoke</a> which may also be a good way of providing read-only access across your org landscape.</p>
<p dir="ltr"><strong>Capability #10</strong> &#8211; The Salesforce Platform’s robust integration technology makes it very easy to integrate point-to-point with other systems.  While it would be recommended for some solutions (Google Maps mashups, etc) I would try to stay away from using Apex as your primary integration technology.  While it is more than capable at handling transformation, error handling, and retries &#8211; these types of requirements should be pushed to middleware if possible. The more that Salesforce is made to be the hub of integration activity, the more time you will spend building, maintaining, and troubleshooting integrations as opposed to building new business value.  This is a trap I have seen many companies fall into.</p>
<p dir="ltr"><strong>Capability #11</strong> &#8211; Rather than using the Salesforce Platform to be your hub of cloud-to-cloud integration activity many companies have moved towards Cloud based Integration-as-a-Service packages.  While not true ESB’s per se, many integration vendors have started providing cloud based solutions for managing your cloud-to-cloud use cases.   Because these solutions are specifically tailored for the Salesforce Platform (and other popular SaaS vendors), the time to build and deploy an integration can be radically reduced as opposed to using the On-Premise ESB.</p>
<p dir="ltr"><strong>Capability #12</strong> &#8211; The cloud service bus can handle service mediation, transformation, routing, error handling, etc to your other cloud based end-points.  Having to build durable and resilient integration solutions inside of Salesforce can be expensive and very complicated.  Middleware should be used where and when possible.</p>
<p><strong>Capability #13</strong> &#8211; Some companies prefer to broker all integrations through their ESB, including Cloud-to-Cloud use cases.  My warnings here are this: the cost of highly resilient ESB’s can be EXTREMELY high.  If the service levels between Salesforce and Workday, for example, must go through your on-premise technology, you maybe shooting yourself in the foot.  Now your “Cloud” solution is piggy-backing on the same technical infrastructure, cost, service levels, and release timeline of your On-Premise solutions.  Tread lightly and make sure to design your Integration Architecture first and foremost about delivering BUSINESS VALUE.</p>
<h2>In Summary</h2>
<p>I was previously an Enterprise Architect working with Service Oriented Architectures before becoming a Salesforce Certified Technical Architect.  When I was first introduced to Salesforce I was surprised to see either a 100% dependency on batch integration technology or a total reluctance to use anything but Real-Time services design.  However one of the reasons I enjoy what I do so much is that I have learned that there is no glass slipper in Salesforce Integration Architecture.  One size does not fit all and no one solution can be the best for all or your requirements.  It is up to you as the architect to analyze, recommend, and implement a variety of integration capabilities that will enable your team, clients, and company to realize the powerful transformation of moving to the Salesforce1 Platform.</p>
<h2>Read the Entire Series</h2>
<ul>
<li><a title="Enterprise Architecture: Single-org versus Multi-org Strategy" href="https://developer.salesforce.com/blogs/developer-relations/2014/10/enterprise-architecture-multi-org-strategy.html">Enterprise Architecture: Single-org versus Multi-org Strategy</a></li>
<li><a title="Designing Enterprise Data Architecture on the Salesforce1 Platform" href="https://developer.salesforce.com/blogs/developer-relations/2014/11/enterprise-data-architecture.html">Designing Enterprise Data Architecture on the Salesforce1 Platform</a></li>
</ul>
<h3>Helpful Resources</h3>
<p><a href="http://www.salesforce.com/us/developer/docs/integration_patterns/integration_patterns_and_practices.pdf" target="_blank">Integration Patterns and Practices</a></p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/SforceBlog?a=Dgz5dBUWv6Y:OI-ClTQD7XE:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=Dgz5dBUWv6Y:OI-ClTQD7XE:qj6IDK7rITs"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=qj6IDK7rITs" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=Dgz5dBUWv6Y:OI-ClTQD7XE:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=Dgz5dBUWv6Y:OI-ClTQD7XE:V_sGLiPBpWU" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=Dgz5dBUWv6Y:OI-ClTQD7XE:F7zBnMyn0Lo"><img src="http://feeds.feedburner.com/~ff/SforceBlog?i=Dgz5dBUWv6Y:OI-ClTQD7XE:F7zBnMyn0Lo" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/SforceBlog?a=Dgz5dBUWv6Y:OI-ClTQD7XE:l6gmwiTKsz0"><img src="http://feeds.feedburner.com/~ff/SforceBlog?d=l6gmwiTKsz0" border="0"></img></a>
</div><img src="//feeds.feedburner.com/~r/SforceBlog/~4/Dgz5dBUWv6Y" height="1" width="1" alt=""/>]]></content:encoded>
			<wfw:commentRss>http://developer.salesforce.com/blogs/developer-relations/2014/11/salesforce-integration-architecture.html/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		<feedburner:origLink>http://developer.salesforce.com/blogs/developer-relations/2014/11/salesforce-integration-architecture.html</feedburner:origLink></item>
	</channel>
</rss>